# Relaci√≥n entre Distribuciones. Convergencia en Distribuci√≥n {#conv}

## Introducci√≥n

En el estudio de los fen√≥menos aleatorios y en la modelizaci√≥n del azar, las distribuciones de probabilidad desempe√±an un papel fundamental. No obstante, en muchas situaciones pr√°cticas no se trabaja con distribuciones exactas, sino con aproximaciones. Comprender c√≥mo se relacionan diferentes distribuciones y bajo qu√© condiciones una puede aproximarse por otra es una herramienta clave en el an√°lisis de datos, en la inferencia estad√≠stica y en la toma de decisiones fundamentadas en incertidumbre.

Este tema aborda dos ideas centrales. Por un lado, el estudio de las **relaciones entre distribuciones** permite simplificar c√°lculos complejos utilizando distribuciones l√≠mite o aproximadas. Por ejemplo, en vez de trabajar directamente con una distribuci√≥n binomial con gran n√∫mero de ensayos, puede resultar mucho m√°s eficiente aproximarla por una normal. Estas aproximaciones son esenciales para desarrollar soluciones pr√°cticas en contextos reales, donde la precisi√≥n absoluta cede paso a la eficiencia computacional y a la interpretabilidad.

Por otro lado, se introduce el concepto de **convergencia en distribuci√≥n**, una noci√≥n fundamental en probabilidad que explica c√≥mo se comportan las distribuciones de ciertas variables aleatorias a medida que se incrementa el tama√±o de la muestra o cambia un par√°metro clave. Esta idea es la base te√≥rica detr√°s del Teorema Central del L√≠mite y sustenta muchas de las t√©cnicas estad√≠sticas modernas, desde la estimaci√≥n mediante remuestreo (bootstrap) hasta el an√°lisis asint√≥tico de estimadores y contrastes.

En el √°mbito del an√°lisis de datos, estas herramientas permiten justificar el uso de modelos simplificados en grandes vol√∫menes de datos, aplicar inferencias sobre par√°metros poblacionales o entender el comportamiento de algoritmos estad√≠sticos. En econom√≠a y empresa, facilitan la toma de decisiones bajo incertidumbre, el an√°lisis de riesgos, la evaluaci√≥n de pol√≠ticas y la previsi√≥n basada en grandes muestras o en distribuciones derivadas de simulaciones.

## Relaciones entre distribuciones

En estad√≠stica y teor√≠a de la probabilidad, es frecuente que una variable aleatoria se modele inicialmente con una cierta distribuci√≥n, pero que para facilitar los c√°lculos o el an√°lisis, se recurra a una **distribuci√≥n aproximada**. Estas relaciones entre distribuciones son particularmente √∫tiles cuando los par√°metros de la distribuci√≥n original se encuentran en ciertos rangos que permiten una buena aproximaci√≥n.

En este apartado se revisan algunas de las **aproximaciones cl√°sicas** m√°s utilizadas, as√≠ como ejemplos pr√°cticos que ilustran sus condiciones de validez.

### Aproximaciones cl√°sicas

**Binomial ‚âà Poisson**

La distribuci√≥n binomial $B(n, p)$ puede aproximarse por una distribuci√≥n de Poisson $\text{Po}(\lambda)$ cuando:

-   $n$ es grande,
-   $p$ es peque√±o,
-   y $\lambda = np$ se mantiene constante.

**Condiciones t√≠picas:**

\- $n \geq 30$

\- $p \leq 0.1$

Esta aproximaci√≥n es √∫til en contextos como la modelizaci√≥n de defectos poco frecuentes en procesos industriales o incidencias poco comunes en un gran n√∫mero de observaciones.

**Binomial ‚âà Normal**

Cuando $n$ es grande, la distribuci√≥n binomial puede aproximarse por una normal:

$$
B(n, p) \approx \mathcal{N}(np, np(1 - p))
$$

**Condiciones t√≠picas:**

\- $np \geq 5$

\- $n(1 - p) \geq 5$

**Correcci√≥n por continuidad:** Para mejorar la aproximaci√≥n, se usa una correcci√≥n por continuidad. Por ejemplo, para calcular $P(X \leq k)$, se utiliza:

$$
P(Y \leq k + 0.5), \quad Y \sim \mathcal{N}(np, np(1-p))
$$

Esta aproximaci√≥n es especialmente √∫til en an√°lisis de proporciones, encuestas y estudios de comportamiento del consumidor.

**Poisson ‚âà Normal**

Si $\lambda$ es suficientemente grande, una variable Poisson puede aproximarse por una normal:

$$
\text{Po}(\lambda) \approx \mathcal{N}(\lambda, \lambda)
$$

**Condici√≥n t√≠pica:** - $\lambda \geq 10$

Este caso aparece en problemas de conteo, como el n√∫mero de llamadas en un centro de atenci√≥n al cliente por minuto o el n√∫mero de transacciones por segundo en una plataforma digital.

```{r echo=FALSE}
# Instalar y cargar DiagrammeR si es necesario
#install.packages("DiagrammeR")    # ejecutar solo si no est√° instalada
library(DiagrammeR)

# Generar el diagrama usando sintaxis DOT de Graphviz
grViz("
digraph {
  rankdir=TB;
  node [shape=ellipse, style=filled, fillcolor=gray95, fontname=Helvetica];
  Bin [label=\"Distribuci√≥n Binomial\"];
  Poi [label=\"Distribuci√≥n Poisson\"];
  Nor [label=\"Distribuci√≥n Normal\"];
  Bin -> Poi [label=\"n grande, p peque√±o, np = constante\"];
  Bin -> Nor [label=\"np ‚â• 5, n(1-p) ‚â• 5\"];
  Poi -> Nor [label=\"Œª ‚â• 10\"];
}
")

```

![](Modelos-continuos_files/aproximaciones%20cl√°sicas.png)

**Figura 1:** Diagrama de las relaciones de aproximaci√≥n entre la distribuci√≥n binomial, Poisson y normal. Las flechas indican que una distribuci√≥n sirve de aproximaci√≥n a otra bajo las condiciones se√±aladas junto a cada flecha. Por ejemplo, una $B(n,p)$ con $n$ muy grande y $p$ muy peque√±o (de modo que $np$ se mantiene aproximadamente constante) se puede aproximar mediante una $Poisson(n p)$; si $np$ y $n(1-p)$ son bastante grandes, $B(n,p)$ se aproxima bien por una normal; y si $\lambda$ es grande, $Poisson(\lambda)$ se aproxima por $\mathcal{N}(\lambda,\lambda)$.

### Condiciones de validez para cada aproximaci√≥n

Las siguientes condiciones son orientativas y ayudan a decidir si una aproximaci√≥n es razonable en la pr√°ctica:

-   **Binomial ‚âà Poisson**: v√°lida cuando el n√∫mero de ensayos $n$ es grande (por ejemplo, $n \geq 30$) y la probabilidad de √©xito $p$ es peque√±a (por ejemplo, $p \leq 0.1$), de forma que $\lambda = np$ se mantenga constante y de valor moderado.

-   **Binomial ‚âà Normal**: se recomienda que tanto $np \geq 5$ como $n(1 - p) \geq 5$. Adem√°s, la correcci√≥n por continuidad mejora significativamente la aproximaci√≥n cuando $n$ no es muy grande.

-   **Poisson ‚âà Normal**: adecuada cuando $\lambda \geq 10$. A mayor valor de $\lambda$, mejor ser√° la aproximaci√≥n, ya que la distribuci√≥n Poisson se vuelve m√°s sim√©trica.

Estas condiciones son de car√°cter pr√°ctico y deben verificarse antes de aplicar la aproximaci√≥n. En caso contrario, se corre el riesgo de obtener resultados imprecisos.

**Aplicaciones pr√°cticas: simplificaci√≥n de c√°lculos, simulaciones, modelizaci√≥n en contexto econ√≥mico y empresarial**

Estas aproximaciones son fundamentales en numerosos entornos de trabajo donde la eficiencia computacional, la rapidez de decisi√≥n y la claridad de interpretaci√≥n son clave. Algunos ejemplos incluyen:

-   **Simplificaci√≥n de c√°lculos**: en vez de trabajar con sumas de probabilidades o combinatorias complicadas (como en la binomial), se puede utilizar una distribuci√≥n m√°s manejable como la normal, con funciones ya integradas en la mayor√≠a de software estad√≠sticos.

-   **Simulaci√≥n de escenarios**: en an√°lisis de datos y econometr√≠a, las simulaciones con miles de repeticiones son comunes. Utilizar distribuciones l√≠mite (como la normal) permite generar datos de forma m√°s r√°pida y con menor coste computacional.

-   **Modelizaci√≥n en econom√≠a y empresa**:

    -   En estudios de mercado, se modelan proporciones (clientes que compran un producto, votantes que eligen una opci√≥n) mediante binomiales, que pueden aproximarse por normales en grandes muestras.
    -   En an√°lisis de riesgos, incidentes o reclamaciones poco frecuentes (seguros, sistemas de calidad) se modelan con Poisson, y si el n√∫mero esperado de eventos es alto, se emplea la normal.
    -   En log√≠stica o producci√≥n, el n√∫mero de errores, fallos o llegadas a un sistema se puede modelar inicialmente con Poisson y luego aproximarse por normal si se cumplen las condiciones adecuadas.

Estas aproximaciones tambi√©n constituyen una introducci√≥n natural al uso de m√©todos asint√≥ticos, fundamentales en t√©cnicas modernas como los contrastes de hip√≥tesis, la inferencia basada en simulaci√≥n y el aprendizaje autom√°tico con grandes vol√∫menes de datos.

**Ejemplos con condiciones de validez**

A continuaci√≥n se presentan algunos ejemplos ilustrativos que muestran c√≥mo aplicar las aproximaciones mencionadas y en qu√© situaciones son v√°lidas.

***Ejemplo 1: Binomial grande, Poisson como aproximaci√≥n***

Supongamos que en una gran f√°brica hay 10,000 productos y la probabilidad de que uno sea defectuoso es de 0.001.

$$
X \sim B(10000, 0.001), \quad \lambda = np = 10
$$

Dado que $n$ es grande y $p$ es peque√±o, podemos aproximar:

$$
X \approx \text{Po}(10)
$$

Esto permite simplificar el c√°lculo de probabilidades como $P(X = 0)$, $P(X \leq 5)$, etc., sin tener que calcular combinatorias.

***Ejemplo 2: Binomial a Normal con correcci√≥n***

Sea $X \sim B(100, 0.4)$. Queremos calcular $P(X \geq 45)$. Dado que $np = 40$ y $n(1 - p) = 60$, se cumple la condici√≥n para usar la normal.

$$
X \approx \mathcal{N}(40, 24)
$$

Aplicamos la correcci√≥n por continuidad:

$$
P(X \geq 45) \approx P(Y \geq 44.5), \quad Y \sim \mathcal{N}(40, 24)
$$

Este tipo de aproximaci√≥n es habitual en estudios de poblaci√≥n donde se analizan porcentajes de respuestas o elecciones en muestras grandes.

***Ejemplo 3: Poisson con par√°metro alto***

Si el n√∫mero de accidentes laborales mensuales sigue una Poisson con $\lambda = 15$, entonces:

$$
X \sim \text{Po}(15) \approx \mathcal{N}(15, 15)
$$

Esto permite usar la normal para construir intervalos de confianza o contrastes, incluso si originalmente se part√≠a de una distribuci√≥n discreta.

Estas relaciones entre distribuciones son esenciales para aplicar herramientas de inferencia estad√≠stica de forma eficiente, especialmente en escenarios donde el volumen de datos es elevado o el tiempo de c√°lculo es limitado. Tambi√©n constituyen la base intuitiva para introducir el concepto de **convergencia en distribuci√≥n**, que ser√° formalizado en el siguiente apartado.

## Convergencia en distribuci√≥n

Hasta ahora hemos visto c√≥mo algunas distribuciones pueden aproximarse por otras bajo ciertas condiciones. Sin embargo, para dar rigor a estas afirmaciones, es necesario entender en qu√© sentido una sucesi√≥n de distribuciones se aproxima a una distribuci√≥n l√≠mite. Esto nos lleva al concepto fundamental de **convergencia de variables aleatorias**, y en particular a la **convergencia en distribuci√≥n**, tambi√©n conocida como *convergencia d√©bil*.

Entre los distintos tipos de convergencia existentes en teor√≠a de la probabilidad, la convergencia en distribuci√≥n es especialmente relevante en estad√≠stica porque:

-   Permite justificar aproximaciones como $B(n, p) \approx \mathcal{N}(np, np(1-p))$ o $\text{Poisson}(\lambda) \approx \mathcal{N}(\lambda, \lambda)$.
-   Aparece de forma natural en resultados fundamentales como el Teorema Central del L√≠mite.
-   Se emplea para estudiar el comportamiento asint√≥tico de estad√≠sticos y estimadores cuando el tama√±o muestral tiende a infinito.
-   Es la base te√≥rica de t√©cnicas modernas como el bootstrap o la validaci√≥n emp√≠rica por simulaci√≥n.

A diferencia de otras formas m√°s fuertes de convergencia, como la convergencia en probabilidad o en media cuadr√°tica, la convergencia en distribuci√≥n no requiere que las variables aleatorias est√©n definidas en un mismo espacio de probabilidad ni que las realizaciones individuales se acerquen. Solo exige que las **funciones de distribuci√≥n acumulada** converjan.

En este apartado, exploraremos primero los distintos tipos de convergencia para situar la convergencia en distribuci√≥n en su contexto te√≥rico. A continuaci√≥n, abordaremos el Teorema Central del L√≠mite, la convergencia de las distribuciones emp√≠ricas, y finalizaremos con aplicaciones relevantes en econom√≠a, empresa y an√°lisis de datos.

### Tipos de convergencia y definici√≥n formal

En teor√≠a de la probabilidad, es fundamental distinguir entre los distintos tipos de convergencia de sucesiones de variables aleatorias. Cada tipo de convergencia expresa una forma diferente en la que una secuencia de variables aleatorias puede aproximarse a una variable aleatoria l√≠mite. Estas nociones son esenciales tanto desde un punto de vista te√≥rico como aplicado, especialmente en estad√≠stica asint√≥tica y modelizaci√≥n del azar.

A continuaci√≥n se presentan los principales tipos de convergencia, ordenados de m√°s fuerte a m√°s d√©bil:

#### Convergencia casi segura (convergencia con probabilidad)

Se dice que una sucesi√≥n de variables aleatorias $X_n$ converge casi seguramente a una variable aleatoria $X$ si:

$$
P\left( \lim_{n \to \infty} X_n = X \right) = 1
$$

Esta forma de convergencia asegura que las realizaciones de $X_n$ se aproximan indefinidamente a las de $X$, salvo en un conjunto de probabilidad cero.

#### Convergencia en probabilidad

Se dice que $X_n \xrightarrow{P} X$ si, para todo $\varepsilon > 0$:

$$
\lim_{n \to \infty} P(|X_n - X| > \varepsilon) = 0
$$

Es decir, la probabilidad de que $X_n$ est√© lejos de $X$ se hace cada vez m√°s peque√±a a medida que $n$ crece.

#### Convergencia en media $r$-√©sima (por ejemplo, en media cuadr√°tica)

Se dice que $X_n \xrightarrow{L^r} X$ si:

$$
\lim_{n \to \infty} \mathbb{E}[|X_n - X|^r] = 0
$$

Un caso particular importante es $r = 2$, conocido como **convergencia en media cuadr√°tica**. Esta convergencia implica la convergencia en probabilidad bajo ciertas condiciones.

#### Convergencia en distribuci√≥n (o en ley)

Se dice que $X_n \xrightarrow{d} X$ si la funci√≥n de distribuci√≥n acumulada $F_{X_n}(x)$ converge puntualmente a $F_X(x)$ en los puntos de continuidad de $F_X$:

$$
\lim_{n \to \infty} F_{X_n}(x) = F_X(x)
$$

Esta es la forma m√°s d√©bil de convergencia y la m√°s utilizada en estad√≠stica inferencial, ya que permite estudiar el comportamiento asint√≥tico de secuencias de variables aleatorias sin requerir convergencia en t√©rminos m√°s fuertes.

#### Relaci√≥n entre los tipos de convergencia

Las implicaciones entre los distintos tipos de convergencia pueden resumirse en el siguiente esquema:

$$
\text{Casi segura} \Rightarrow \text{En probabilidad} \Rightarrow \text{En distribuci√≥n}
$$

Adem√°s:

$$
\text{En media cuadr√°tica ($L^2$)} \Rightarrow \text{En probabilidad}
$$

Sin embargo, las implicaciones no se dan en sentido contrario: - La convergencia en distribuci√≥n no implica convergencia en probabilidad. - La convergencia en probabilidad no implica convergencia casi segura ni en media. - La convergencia en distribuci√≥n puede darse incluso si $X_n$ y $X$ no est√°n definidas en el mismo espacio de probabilidad.

Esta jerarqu√≠a de convergencias proporciona el marco formal necesario para entender los resultados asint√≥ticos y las aproximaciones entre distribuciones que estudiaremos en los siguientes apartados.

```{r, echo=FALSE, fig.align='center'}
library(DiagrammeR)

grViz("
digraph jerarquia_convergencia {
  graph [rankdir=TB, layout=dot]

  # Nodos
  cs [label = 'Convergencia casi segura', shape = ellipse, style = filled, fillcolor = gray95]
  mq [label = 'Convergencia en media cuadr√°tica (L¬≤)', shape = ellipse, style = filled, fillcolor = gray95]
  cp [label = 'Convergencia en probabilidad', shape = ellipse, style = filled, fillcolor = gray95]
  cd [label = 'Convergencia en distribuci√≥n', shape = ellipse, style = filled, fillcolor = gray95]

  # Agrupar nodos en el mismo nivel
  {rank = same; cs; mq}

  # Flechas
  cs -> cp
  mq -> cp
  cp -> cd
}
")

```

Figura 2. Jerarqu√≠a entre tipos de convergencia

### Teorema Central del L√≠mite (TCL)

El **Teorema Central del L√≠mite (TCL)** es uno de los resultados m√°s importantes de la teor√≠a de la probabilidad y constituye la base te√≥rica de muchas t√©cnicas estad√≠sticas. Justifica por qu√© en muchos contextos, incluso cuando los datos individuales no siguen una distribuci√≥n normal, la distribuci√≥n de ciertos estad√≠sticos tiende a ser normal cuando el tama√±o muestral es grande.

**Definici√≥n**

Sea $X_1, X_2, \dots, X_n$ una sucesi√≥n de variables aleatorias independientes e id√©nticamente distribuidas (i.i.d.) con:

-   $\mathbb{E}(X_i) = \mu$
-   $\text{Var}(X_i) = \sigma^2 < \infty$

Entonces, la variable aleatoria:

$$
Z_n = \frac{\overline{X}_n - \mu}{\sigma / \sqrt{n}} \quad \text{donde} \quad \overline{X}_n = \frac{1}{n} \sum_{i=1}^n X_i
$$

converge en distribuci√≥n a una normal est√°ndar:

$$
Z_n \xrightarrow{d} \mathcal{N}(0,1)
$$

**Interpretaci√≥n intuitiva**

Aunque los datos individuales $X_i$ no est√©n normalmente distribuidos, al tomar una media de muchas observaciones (muestra grande), la distribuci√≥n de esa media se aproxima a una normal. Cuanto mayor es el tama√±o de la muestra, mejor es la aproximaci√≥n.

Por simplicicdad, en la definici√≥n se ha supuesto que las medias y varianzas de las variables aleatorias eran iguales, sin embargo ese supuesto no es necesario, y se puede hacer con cualquier media o varianza. En el siguiente link se explica muy claramente esta afirmaci√≥n. (.<https://bookdown.org/aquintela/EBE/el-teorema-central-del-limite.html>)

Este resultado explica por qu√© la distribuci√≥n normal aparece con tanta frecuencia en estad√≠stica, incluso cuando los fen√≥menos originales no son normales.

**Ejemplos de aplicaci√≥n en an√°lisis de datos y econom√≠a**

-   **Encuestas**: al calcular la media de respuestas de una muestra grande (por ejemplo, gasto mensual), el TCL permite aproximar la distribuci√≥n de la media por una normal, lo cual es clave para construir intervalos de confianza y realizar contrastes de hip√≥tesis.
-   **An√°lisis financiero**: al modelar rendimientos medios de activos financieros en periodos largos, el TCL justifica el uso de herramientas normales, incluso si los rendimientos individuales tienen colas pesadas o asimetr√≠a.
-   **Control de calidad**: en procesos industriales donde se toma una muestra de productos, el TCL permite estimar con precisi√≥n el valor medio del par√°metro de inter√©s (por ejemplo, peso, tama√±o, tiempo de fabricaci√≥n).

**Relaci√≥n con las aproximaciones vistas**

El Teorema Central del L√≠mite **justifica las aproximaciones** que hemos visto anteriormente, como:

-   $B(n,p) \approx \mathcal{N}(np, np(1-p))$
-   $\text{Poisson}(\lambda) \approx \mathcal{N}(\lambda, \lambda)$

En ambos casos, lo que subyace es que se est√°n sumando muchas variables aleatorias independientes (Bernoulli o indicadores de ocurrencia), y su suma se aproxima a una normal, tal como predice el TCL.

Este teorema es esencial en inferencia estad√≠stica porque permite aplicar m√©todos basados en la normalidad en una gran variedad de contextos, incluso cuando los datos originales no son normales.

### Convergencia de distribuciones emp√≠ricas $F_n$

En estad√≠stica aplicada y en ciencia de datos es habitual trabajar con muestras y estimar la distribuci√≥n de una variable aleatoria a partir de sus valores observados. Una herramienta fundamental para ello es la **funci√≥n de distribuci√≥n emp√≠rica**, que permite aproximar la distribuci√≥n te√≥rica a partir de una muestra.

**Distribuci√≥n emp√≠rica** $F_n$

Dada una muestra aleatoria $X_1, X_2, \dots, X_n$ de una variable aleatoria $X$, se define la **funci√≥n de distribuci√≥n emp√≠rica** como:

$$
F_n(x) = \frac{1}{n} \sum_{i=1}^n \mathbb{I}_{\{X_i \leq x\}}
$$

donde $\mathbb{I}_{\{X_i \leq x\}}$ es una funci√≥n indicadora que vale 1 si $X_i \leq x$ y 0 en caso contrario.

$F_n(x)$ representa la proporci√≥n de valores de la muestra que son menores o iguales que $x$, y constituye una estimaci√≥n natural de la funci√≥n de distribuci√≥n verdadera $F(x)$.

**Convergencia de** $F_n$ a $F$

A medida que aumenta el tama√±o muestral $n$, la funci√≥n de distribuci√≥n emp√≠rica $F_n(x)$ converge a la verdadera funci√≥n de distribuci√≥n $F(x)$. Esta convergencia se produce **uniformemente en todos los puntos** $x$, y se puede expresar formalmente como:

$$
\sup_x |F_n(x) - F(x)| \xrightarrow{a.s.} 0
$$

Esta es una forma fuerte de convergencia (convergencia casi segura), y garantiza que, con suficiente tama√±o muestral, la distribuci√≥n emp√≠rica se aproxima arbitrariamente bien a la real.

**Teorema de Glivenko-Cantelli (opcional)**

Este resultado formaliza la convergencia uniforme de la distribuci√≥n emp√≠rica:

Sea $X_1, X_2, \dots, X_n$ una muestra i.i.d. de una variable con funci√≥n de distribuci√≥n $F$. Entonces:

$$
\sup_x |F_n(x) - F(x)| \xrightarrow{a.s.} 0 \quad \text{cuando } n \to \infty
$$

Este teorema garantiza que, con probabilidad 1, la distribuci√≥n emp√≠rica converge uniformemente a la distribuci√≥n real a medida que el tama√±o muestral crece.

**Aplicaci√≥n pr√°ctica**

La distribuci√≥n emp√≠rica es ampliamente utilizada en:

-   **Visualizaci√≥n de datos**: al comparar $F_n(x)$ con distribuciones te√≥ricas (gr√°ficas Q-Q o P-P).
-   **Simulaci√≥n y bootstrap**: se parte de $F_n$ para generar nuevas muestras (remuestreo con reemplazo).
-   **Contrastes no param√©tricos**: como el test de Kolmog√≥rov-Smirnov, que mide la distancia entre $F_n$ y una distribuci√≥n te√≥rica.

### Ejemplos y visualizaciones

**Ejemplo 1: Convergencia en media cuadr√°tica pero no casi seguramente**

Este ejemplo muestra que la **convergencia en media cuadr√°tica** no implica necesariamente la **convergencia casi segura**. Simularemos una sucesi√≥n de variables aleatorias $X_n$ definida de la siguiente manera:

$$
X_n =
\begin{cases}
1 & \text{con probabilidad } \frac{1}{n} \\
0 & \text{con probabilidad } 1 - \frac{1}{n}
\end{cases}
$$

Esto significa que a medida que $n$ crece, la probabilidad de que $X_n = 1$ disminuye, pero **nunca es cero**.

¬øQu√© vamos a observar?

1.  **Convergencia en media cuadr√°tica**: la esperanza $\mathbb{E}(X_n^2) = \mathbb{E}(X_n) = \frac{1}{n} \to 0$. Veremos que el promedio de los cuadrados de los $X_n$ tiende a 0.

2.  **¬øConvergencia casi segura?**: veremos que, aunque los valores de $X_n$ son mayoritariamente ceros, **nunca dejan de aparecer algunos unos** (saltos). Por tanto, no hay una realizaci√≥n fija hacia la que todos los $X_n$ tiendan.

***Simulaci√≥n en R***

```{r, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(123)

# Tama√±o de la sucesi√≥n
n <- 1000

# Generamos la sucesi√≥n: X_n ~ Bernoulli(1/n)
Xn <- rbinom(n, size = 1, prob = 1 / (1:n))

# Media cuadr√°tica acumulada
media_cuadratica <- sapply(1:n, function(i) mean(Xn[1:i]^2))

# N√∫mero acumulado de 'unos'
saltos_acumulados <- cumsum(Xn)

# Dibujamos los gr√°ficos
par(mfrow = c(1, 2), mar = c(4, 4, 2, 1))

# Gr√°fico 1: convergencia en media cuadr√°tica
plot(1:n, media_cuadratica, type = "l", lwd = 2, col = "blue",
     main = "Convergencia en media cuadr√°tica",
     xlab = "n", ylab = "Media cuadr√°tica acumulada")
abline(h = 0, col = "gray", lty = 2)

# Gr√°fico 2: acumulaci√≥n de saltos (unos)
plot(1:n, saltos_acumulados, type = "l", lwd = 2, col = "red",
     main = "Aparici√≥n de 'unos'",
     xlab = "n", ylab = "N√∫mero acumulado de unos")

```

**Conclusi√≥n**

-   **S√≠ hay convergencia en media cuadr√°tica**:\
    En el gr√°fico de la izquierda observamos que la media de los cuadrados de los $X_n$ tiende a 0 cuando $n$ crece. Esto confirma que la sucesi√≥n $X_n$ converge a 0 en media cuadr√°tica.

-   **No hay convergencia casi segura**:\
    En el gr√°fico de la derecha vemos que, aunque los unos aparecen cada vez con menos frecuencia, **nunca desaparecen del todo**. Es decir, por grande que sea $n$, siempre existe una probabilidad (aunque peque√±a) de que $X_n = 1$. Por tanto, no hay una realizaci√≥n de la sucesi√≥n que se mantenga fija a partir de cierto punto, y eso **viola la definici√≥n de convergencia casi segura**.

Este ejemplo demuestra que **la convergencia en media cuadr√°tica no implica convergencia casi segura**. Es una excelente ilustraci√≥n de la jerarqu√≠a entre tipos de convergencia y de por qu√© no deben confundirse.

**Ejemplo 2: El m√©todo bootstrap como aplicaci√≥n de la convergencia de** $F_n$

El **m√©todo bootstrap** es una t√©cnica de remuestreo que permite estimar la distribuci√≥n de un estad√≠stico (como la media, la mediana, o la varianza) **a partir de una √∫nica muestra**. En lugar de asumir una distribuci√≥n te√≥rica concreta, se utiliza la **distribuci√≥n emp√≠rica** $F_n$ como una aproximaci√≥n de la distribuci√≥n real de los datos.

La idea central del bootstrap es que si $F_n$ se aproxima bien a la distribuci√≥n verdadera $F$, entonces al generar nuevas muestras (con reemplazo) a partir de los datos observados, podemos simular el comportamiento del estad√≠stico como si estuvi√©ramos muestreando del modelo original.

**¬øQu√© se persigue?** Queremos estimar la **distribuci√≥n de la media muestral** sin conocer la distribuci√≥n de la poblaci√≥n original. Para ello:

-   Tomamos una muestra original de tama√±o $n$.
-   Generamos muchas **muestras bootstrap** de tama√±o $n$, remuestreando con reemplazo de la muestra original.
-   Calculamos la **media** en cada una de esas muestras bootstrap.
-   Observamos c√≥mo se distribuyen esas medias y comparamos su forma con la predicha por el Teorema Central del L√≠mite.

Se **observa** que la distribuci√≥n de las medias bootstrap:

-   Se aproxima a una **normal** cuando $n$ es suficientemente grande.
-   Tiene una **dispersi√≥n similar** a la que tendr√≠a la distribuci√≥n real del estad√≠stico si se repitiera el muestreo desde la poblaci√≥n.

**¬øCu√°l es el resultado?** El bootstrap **simula** la distribuci√≥n del estad√≠stico sin necesidad de conocer la verdadera distribuci√≥n de los datos. Y, gracias a la **convergencia en distribuci√≥n** y al hecho de que $F_n \to F$, esta aproximaci√≥n es **cada vez m√°s precisa** conforme aumenta el tama√±o muestral.

El ejemplo que sigue permite visualizar c√≥mo el bootstrap se apoya en la distribuci√≥n emp√≠rica y en el Teorema Central del L√≠mite para generar inferencias v√°lidas de forma no param√©trica.

***Resultados de la estimaci√≥n de la distribuci√≥n de la media mediante bootstrap***

Vamos a aplicar el m√©todo bootstrap para estimar la distribuci√≥n de la media muestral a partir de una sola muestra. Usaremos una variable con distribuci√≥n desconocida (en este caso una **exponencial**) y observaremos c√≥mo el bootstrap reproduce la forma de la distribuci√≥n del estad√≠stico.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(123)

# Tama√±o de la muestra original
n <- 50

# Generamos una muestra original de una exponencial con media 10
muestra_original <- rexp(n, rate = 1/10)

# N√∫mero de r√©plicas bootstrap
B <- 5000

# Generamos las medias bootstrap
medias_bootstrap <- replicate(B, mean(sample(muestra_original, size = n, replace = TRUE)))

# Dibujamos el histograma
hist(medias_bootstrap, breaks = 50, probability = TRUE,
     col = "skyblue", border = "white",
     main = "Distribuci√≥n bootstrap de la media",
     xlab = "Media bootstrap")

# A√±adimos una curva normal te√≥rica para comparar (basada en la muestra original)
media_hat <- mean(muestra_original)
sd_hat <- sd(muestra_original) / sqrt(n)
curve(dnorm(x, mean = media_hat, sd = sd_hat),
      add = TRUE, col = "darkred", lwd = 2)
legend("topright", legend = c("Histograma", "Aproximaci√≥n normal"),
       fill = c("skyblue", NA), border = c("white", NA),
       lty = c(NA, 1), col = c("skyblue", "darkred"), bty = "n")

```

**Interpretaci√≥n** La distribuci√≥n de las medias bootstrap (en azul) se ajusta notablemente bien a una distribuci√≥n normal (curva roja), a pesar de que la variable original sigue una distribuci√≥n exponencial (asim√©trica y no normal).

Esto confirma el poder del Teorema Central del L√≠mite y la validez del uso de $F_n$ (la distribuci√≥n emp√≠rica) para aproximar la distribuci√≥n de un estad√≠stico.

Este procedimiento permite construir intervalos de confianza o estimar errores est√°ndar sin necesidad de asumir una distribuci√≥n poblacional conocida.

***Ejemplo 3: Comparaci√≥n de*** $F_n$ ***y*** $F$ ***con el test de Kolmog√≥rov-Smirnov***

Este ejemplo muestra c√≥mo se mide formalmente la **diferencia entre la distribuci√≥n emp√≠rica** $F_n(x)$ y la distribuci√≥n te√≥rica $F(x)$ usando el **estad√≠stico de Kolmog√≥rov-Smirnov**, definido como:

$$
D_n = \sup_x |F_n(x) - F(x)|
$$

Queremos observar c√≥mo $D_n$ disminuye al aumentar el tama√±o muestral, lo que ilustra la **convergencia uniforme** de $F_n$ a $F$, como afirma el Teorema de Glivenko-Cantelli.

***Simulaci√≥n en R***

```{r kolmogorov-smirnov-demo, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(42)

# Tama√±os muestrales
n_values <- c(10, 30, 100)
x_vals <- seq(0, 1, length.out = 200)
F_teorica <- punif(x_vals)

# Funci√≥n para obtener F_n(x)
F_empirica <- function(sample, x_vals) {
  sapply(x_vals, function(x) mean(sample <= x))
}

# Muestras y distancias D
samples <- lapply(n_values, function(n) runif(n))
emp_cdfs <- lapply(samples, F_empirica, x_vals = x_vals)
d_stats <- mapply(function(f_n, f) max(abs(f_n - f)), emp_cdfs, MoreArgs = list(f = F_teorica))

# Graficar resultados
par(mfrow = c(1, 3), mar = c(4, 4, 2, 1))
for (i in seq_along(n_values)) {
  plot(x_vals, F_teorica, type = "l", lwd = 2, col = "black",
       main = paste("n =", n_values[i]),
       xlab = "x", ylab = "F(x)", ylim = c(0, 1))
  lines(x_vals, emp_cdfs[[i]], col = "steelblue", lwd = 2)
  legend("bottomright", legend = c("F(x) te√≥rica", "F_n(x) emp√≠rica"),
         col = c("black", "steelblue"), lty = 1, bty = "n")
  text(0.05, 0.9, paste("D =", round(d_stats[i], 3)), pos = 4)
}
```

**Interpretaci√≥n**

-   El **estad√≠stico** $D$ mide la **m√°xima diferencia vertical** entre la funci√≥n de distribuci√≥n emp√≠rica $F_n(x)$ y la funci√≥n de distribuci√≥n te√≥rica $F(x)$.
-   En los gr√°ficos se observa que, a medida que aumenta el tama√±o de la muestra $n$, la curva emp√≠rica $F_n(x)$ se ajusta cada vez mejor a la curva te√≥rica $F(x)$.
-   La **distancia** $D$ disminuye con $n$, lo que ilustra de forma emp√≠rica el **Teorema de Glivenko-Cantelli**, que afirma que $F_n(x) \to F(x)$ uniformemente con probabilidad 1.

Este ejemplo muestra c√≥mo la convergencia de la distribuci√≥n emp√≠rica no solo es una herramienta te√≥rica, sino que tiene aplicaciones pr√°cticas directas como el **test de Kolmog√≥rov-Smirnov**, muy utilizado en an√°lisis no param√©trico y validaci√≥n de modelos.

***Ejemplo 4 del TCL: Ingresos de clientes (distribuci√≥n exponencial)***

Supongamos que los ingresos diarios de ciertos clientes en una empresa siguen una distribuci√≥n **exponencial** con media 100. Esta distribuci√≥n es muy asim√©trica, lo cual la aleja de una normal.

Queremos ver c√≥mo se comporta la distribuci√≥n de la media muestral para distintos tama√±os de muestra.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(123)

# Par√°metros
n_muestras <- 10000
lambda <- 1/100  # media = 1/lambda = 100
tamanyos <- c(5, 30, 100)

# Funci√≥n para simular medias
simula_medias <- function(n) {
  replicate(n_muestras, mean(rexp(n, rate = lambda)))
}

# Simulaciones
medias_5 <- simula_medias(5)
medias_30 <- simula_medias(30)
medias_100 <- simula_medias(100)

# Dibujamos histogramas
par(mfrow = c(1, 3), mar = c(4, 4, 2, 1))
hist(medias_5, breaks = 50, col = "lightblue", main = "n = 5", xlab = "Media muestral")
hist(medias_30, breaks = 50, col = "lightgreen", main = "n = 30", xlab = "Media muestral")
hist(medias_100, breaks = 50, col = "lightcoral", main = "n = 100", xlab = "Media muestral")
```

*Interpretaci√≥n:* aunque la distribuci√≥n original es claramente no normal (muy asim√©trica), la distribuci√≥n de la media muestral se va aproximando a una normal a medida que aumenta el tama√±o de la muestra, como predice el TCL.

***Ejemplo 5 del TCL: Tiempo de respuesta de clientes (distribuci√≥n uniforme)***

Supongamos ahora que el tiempo de respuesta de los clientes ante una campa√±a publicitaria sigue una distribuci√≥n **uniforme entre 0 y 10 minutos**. Esta distribuci√≥n es sim√©trica pero no normal. Observamos c√≥mo evoluciona la distribuci√≥n de la media muestral a medida que aumenta el tama√±o de la muestra.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Fijamos semilla
set.seed(123)

# N√∫mero de repeticiones
n_muestras <- 10000

# Funci√≥n para simular medias muestrales de una uniforme
simula_medias_uni <- function(n) {
  replicate(n_muestras, mean(runif(n, min = 0, max = 10)))
}

# Simulaciones para distintos tama√±os muestrales
medias_uni_5 <- simula_medias_uni(5)
medias_uni_30 <- simula_medias_uni(30)
medias_uni_100 <- simula_medias_uni(100)

# Dibujamos histogramas
par(mfrow = c(1, 3), mar = c(4, 4, 2, 1))
hist(medias_uni_5, breaks = 50, col = "skyblue", main = "n = 5", xlab = "Media muestral")
hist(medias_uni_30, breaks = 50, col = "seagreen", main = "n = 30", xlab = "Media muestral")
hist(medias_uni_100, breaks = 50, col = "salmon", main = "n = 100", xlab = "Media muestral")
```

*Interpretaci√≥n:* aunque la variable original (tiempo de respuesta) no sigue una normal sino una distribuci√≥n uniforme, la distribuci√≥n de la media muestral se aproxima progresivamente a una distribuci√≥n normal a medida que se incrementa el tama√±o de la muestra.

Esto confirma emp√≠ricamente el Teorema Central del L√≠mite y justifica por qu√© podemos utilizar herramientas basadas en la normalidad (como intervalos de confianza o contrastes) incluso con variables originales no normales, siempre que el tama√±o de muestra sea suficientemente grande.

***Ejemplo 6 de convergencia en distribuci√≥n: Distribuci√≥n normal est√°ndar***

A continuaci√≥n ilustramos mediante una simulaci√≥n en R c√≥mo la funci√≥n de distribuci√≥n emp√≠rica $F_n$ se aproxima a la funci√≥n de distribuci√≥n te√≥rica $F$ al aumentar el tama√±o muestral, tal como garantiza el **Teorema de Glivenko-Cantelli**.

Generamos tres muestras de tama√±os crecientes $n = 10, 50, 500$ de una distribuci√≥n normal est√°ndar, y comparamos sus funciones de distribuci√≥n emp√≠ricas $F_n(x)$ con la distribuci√≥n acumulada te√≥rica $\Phi(x)$.

```{r dist-empirica-vs-teorica, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(42)

# Tama√±os muestrales
n_vals <- c(10, 50, 500)
colores <- c("red", "blue", "forestgreen")

# Secuencia de puntos
x_vals <- seq(-3, 3, length.out = 200)
F_teorica <- pnorm(x_vals)  # distribuci√≥n acumulada N(0,1)

# Graficar F_n para diferentes tama√±os
par(mfrow = c(1, 3), mar = c(4, 4, 2, 1))
for (i in seq_along(n_vals)) {
  n <- n_vals[i]
  muestra <- rnorm(n)
  F_empirica <- ecdf(muestra)
  
  plot(x_vals, F_teorica, type = "l", lwd = 2, col = "black",
       main = paste("n =", n), ylab = "F(x)", xlab = "x", ylim = c(0, 1))
  lines(F_empirica, col = colores[i], lwd = 2)
  legend("topleft", legend = c("F(x) te√≥rica", "F_n(x) emp√≠rica"),
         col = c("black", colores[i]), lty = 1, bty = "n", cex = 0.9)
}
```

**Interpretaci√≥n:** En los tres gr√°ficos se comparan las funciones de distribuci√≥n emp√≠ricas $F_n(x)$ con la funci√≥n te√≥rica $\Phi(x)$ de la normal est√°ndar, para distintos tama√±os muestrales $n = 10, 50, 500$.

-   Cuando $n = 10$, la funci√≥n emp√≠rica presenta **saltos y desviaciones notables** respecto a la distribuci√≥n te√≥rica.
-   Con $n = 50$, los saltos son menores y el ajuste mejora, aunque sigue habiendo variabilidad.
-   A partir de $n = 500$, $F_n(x)$ se **aproxima muy bien a** $\Phi(x)$, y apenas se distinguen visualmente.

Este comportamiento confirma el **Teorema de Glivenko-Cantelli**, que garantiza que la funci√≥n de distribuci√≥n emp√≠rica **converge uniformemente** a la verdadera funci√≥n de distribuci√≥n cuando el tama√±o de la muestra tiende a infinito.

Este ejemplo tambi√©n refuerza la utilidad de $F_n(x)$ en estad√≠stica aplicada, ya que permite aproximar la distribuci√≥n de una variable aleatoria a partir de observaciones reales, incluso cuando se desconoce su forma exacta.

### Aplicaciones en Econom√≠a, empresa y an√°lisis de datos

El concepto de **convergencia en distribuci√≥n** no es solo una herramienta te√≥rica, sino que se encuentra en el n√∫cleo de muchas aplicaciones modernas en an√°lisis de datos, econom√≠a y empresa. Su utilidad principal radica en que permite justificar el uso de distribuciones l√≠mite (como la normal) para estad√≠sticos muestrales, incluso cuando los datos originales no siguen distribuciones conocidas.

A continuaci√≥n se presentan algunas aplicaciones representativas:

**1. Justificaci√≥n asint√≥tica de contrastes y estimadores**

Muchos procedimientos de inferencia estad√≠stica, como los **contrastres de hip√≥tesis** y los **intervalos de confianza**, se basan en el hecho de que ciertos estad√≠sticos convergen en distribuci√≥n a una normal. Por ejemplo:

-   La media muestral $\overline{X}$ converge a $\mathcal{N}(\mu, \sigma^2/n)$ bajo condiciones generales (por el TCL).
-   En regresi√≥n lineal, los estimadores de m√≠nimos cuadrados ordinarios convergen en distribuci√≥n a una normal multivariante.

Esto permite aplicar resultados normales incluso si la poblaci√≥n no es normal, siempre que el tama√±o muestral sea grande.

**2. Bootstrap y remuestreo**

El m√©todo **bootstrap** se basa en el principio de aproximar la distribuci√≥n en el muestreo de un estad√≠stico (media, mediana, etc.) a trav√©s de remuestreo con reemplazo. Bajo ciertas condiciones, se demuestra que la distribuci√≥n bootstrap **converge en distribuci√≥n** a la misma distribuci√≥n l√≠mite que tendr√≠a el estad√≠stico original.

Este enfoque es especialmente √∫til en:

-   Estimaci√≥n de errores est√°ndar sin f√≥rmulas anal√≠ticas.
-   Construcci√≥n de intervalos de confianza en muestras peque√±as.
-   Evaluaci√≥n de la robustez de estimadores.

**3. Evaluaci√≥n de pol√≠ticas econ√≥micas basada en grandes muestras**

En estudios de impacto de pol√≠ticas p√∫blicas o programas sociales, se suele estimar el efecto medio del tratamiento (por ejemplo, un subsidio o reforma). Si se cuenta con una muestra grande, la distribuci√≥n del estimador converge a una normal, lo que permite construir intervalos de confianza o realizar contrastes utilizando esta distribuci√≥n l√≠mite, incluso si la distribuci√≥n original de los datos es asim√©trica o presenta colas pesadas.

**4. Inferencia para big data**

En contextos de **grandes vol√∫menes de datos** (por ejemplo, comportamiento de usuarios, datos financieros de alta frecuencia, registros administrativos masivos), no siempre se conoce la distribuci√≥n exacta de las variables de inter√©s. Sin embargo, gracias a la convergencia en distribuci√≥n, es posible:

-   Utilizar inferencia basada en resultados asint√≥ticos.
-   Justificar la validez de algoritmos de aprendizaje estad√≠stico.
-   Evaluar el comportamiento de modelos de predicci√≥n en muestras grandes.

La convergencia en distribuci√≥n permite aplicar resultados normales y m√©todos de simulaci√≥n en una amplia gama de problemas econ√≥micos y empresariales, facilitando el an√°lisis de fen√≥menos complejos incluso cuando las distribuciones originales son desconocidas o dif√≠ciles de manejar.

## Ejercicios pr√°cticos

**Ejercicio 1. Aproximaci√≥n de la binomial por la distribuci√≥n de Poisson**

Supongamos que en una f√°brica se producen piezas electr√≥nicas con una probabilidad de defecto del 1%. Si se inspecciona una muestra de 100 piezas:

-   Sea $X \sim B(100, 0.01)$ el n√∫mero de piezas defectuosas.
-   Queremos calcular $P(X = 2)$, es decir, la probabilidad de encontrar exactamente 2 defectuosos.

Se pide:

1.  Calcular $P(X = 2)$ exactamente usando la f√≥rmula de la binomial.
2.  Aproximar $P(X = 2)$ usando una distribuci√≥n de Poisson.
3.  Comparar ambos resultados e interpretar.

**Soluci√≥n**

*1. C√°lculo exacto con la distribuci√≥n binomial:*

$$
P(X = 2) = \binom{100}{2} \cdot (0.01)^2 \cdot (0.99)^{98}
$$

Calculamos:

-   $\binom{100}{2} = \frac{100 \cdot 99}{2} = 4950$
-   $(0.01)^2 = 0.0001$
-   $(0.99)^{98} \approx 0.366$

Entonces:

$$
P(X = 2) \approx 4950 \cdot 0.0001 \cdot 0.366 = 0.181
$$

*2. Aproximaci√≥n con una distribuci√≥n de Poisson:*

Usamos que si $X \sim B(n, p)$, con $n$ grande y $p$ peque√±o, entonces:

$$
X \approx \text{Poisson}(\lambda = np) = \text{Poisson}(1)
$$

Entonces:

$$
P(X = 2) \approx \frac{1^2}{2!} e^{-1} = \frac{1}{2} \cdot \frac{1}{e} \approx 0.184
$$

Como resultado se ha obtenido:

-   Exacto (binomial): $P(X = 2) \approx 0.181$
-   Aproximado (Poisson): $P(X = 2) \approx 0.184$

La diferencia es m√≠nima: la aproximaci√≥n de Poisson **simplifica los c√°lculos** y es muy aceptable en este caso, ya que $n = 100$ es suficientemente grande y $p = 0.01$ es peque√±o.

**Interpretaci√≥n:** Este ejercicio muestra c√≥mo aplicar la **aproximaci√≥n binomial ‚Üí Poisson**, una herramienta pr√°ctica cuando el n√∫mero de ensayos es grande y la probabilidad de √©xito es peque√±a. Es √∫til en contextos donde calcular la binomial exacta puede ser costoso, y demuestra c√≥mo las relaciones entre distribuciones permiten **simplificar problemas sin perder precisi√≥n significativa**.

**Ejercicio 2. Aproximaci√≥n de la binomial por la distribuci√≥n normal**

Una empresa realiza un estudio sobre el cumplimiento puntual de sus env√≠os. Se sabe que, hist√≥ricamente, el 80% de los pedidos se entregan a tiempo. Se selecciona una muestra aleatoria de 100 env√≠os.

Sea $X \sim B(100, 0.8)$ el n√∫mero de pedidos entregados puntualmente.

Queremos estimar la probabilidad de que **al menos 85 pedidos lleguen a tiempo**, es decir, $P(X \geq 85)$, usando:

1.  La f√≥rmula exacta de la binomial (solo se plantear√°, no se calcular√° a mano).
2.  La distribuci√≥n normal **sin correcci√≥n por continuidad**.
3.  La distribuci√≥n normal **con correcci√≥n por continuidad**.
4.  Comparaci√≥n e interpretaci√≥n.

**Soluci√≥n**

**Par√°metros de la binomial:**

$$
n = 100, \quad p = 0.8, \quad \mu = np = 80, \quad \sigma = \sqrt{np(1 - p)} = \sqrt{100 \cdot 0.8 \cdot 0.2} = \sqrt{16} = 4
$$

*1. C√°lculo exacto (no se desarrolla):*

$$
P(X \geq 85) = \sum_{x=85}^{100} \binom{100}{x} (0.8)^x (0.2)^{100 - x}=0.0951
$$

```{r}
# Par√°metros
n <- 100
p <- 0.8

# C√°lculo exacto de P(X >= 85)
prob_exacta <- pbinom(84, size = n, prob = p, lower.tail = FALSE)
prob_exacta

```

Es complejo para hacerlo a mano ‚áí se recurre a una aproximaci√≥n (si se hiciera a mano habr√≠a que calcular 16 probabilidades individuales para agregarlas, por lo que es muy laborioso y se muestra con R).

*2. Aproximaci√≥n normal sin correcci√≥n por continuidad:*

Explicaci√≥n del c√°lculo con distribuci√≥n normal

Para aproximar la probabilidad $P(X \geq 85)$ cuando $X \sim B(100, 0.8)$, se utiliza una distribuci√≥n normal con la misma media y desviaci√≥n t√≠pica:

-   $\mu = np = 100 \cdot 0.8 = 80$
-   $\sigma = \sqrt{np(1 - p)} = \sqrt{100 \cdot 0.8 \cdot 0.2} = \sqrt{16} = 4$

En este caso (sin correci√≥n por continuidad), se sustituye directamente la binomial por una normal:

$$
P(X \geq 85) \approx P(Z \geq \frac{85 - 80}{4}) = P(Z \geq 1.25)
$$

Buscando en la tabla de la normal est√°ndar:

$$
P(Z \geq 1.25) = 1 - F(1.25) \approx 1 - 0.8944 = 0.1056
$$

*3. Aproximaci√≥n normal con correcci√≥n por continuidad:*

La correcci√≥n por continuidad consiste en ajustar el valor discreto al entorno continuo. Como queremos $P(X \geq 85)$, pasamos a:

$$
P(Y \geq 84.5), \quad \text{donde } Y \sim \mathcal{N}(80, 4)
$$

Entonces:

$$
Z = \frac{84.5 - 80}{4} = 1.125
$$

$$
P(Z \geq 1.125) = 1 - F(1.125) \approx 1 - 0.8690 = 0.1310
$$

::: {#correccion_continuidad .note}
**Nota did√°ctica: ¬øPor qu√© se aplica la correcci√≥n por continuidad?**

Cuando usamos la distribuci√≥n normal para *aproximar una binomial*, debemos tener en cuenta que:

-   La binomial es *discreta*.
-   La normal es *continua*.

Por ejemplo:

$$
P(X \geq 85) = P(X = 85) + P(X = 86) + \cdots + P(X = 100)
$$

En la normal se calcula el √°rea bajo la curva, por eso se sustituye por:

$$
P(X \geq 85) \approx P(Y \geq 84.5)
$$

**Regla pr√°ctica:**

| Binomial      | Normal (con correcci√≥n)    |
|---------------|----------------------------|
| $P(X \geq k)$ | $P(Y \geq k - 0.5)$        |
| $P(X \leq k)$ | $P(Y \leq k + 0.5)$        |
| $P(X = k)$    | $P(k - 0.5 < Y < k + 0.5)$ |
:::

La correcci√≥n mejora el ajuste porque tiene en cuenta que la binomial toma valores discretos, mientras que la normal es continua.

$$
Z = \frac{85 - 80}{4} = 1.25
$$

$$
P(X \geq 85) \approx P(Z \geq 1.25) = 1 - F(1.25) \approx 1 - 0.8944 = 0.1056
$$

$$
P(X \geq 85) \approx P(Y \geq 84.5), \quad Y \sim \mathcal{N}(80, 16)
$$

$$
Z = \frac{84.5 - 80}{4} = 1.125
$$

$$
P(Z \geq 1.125) = 1 - \Phi(1.125) \approx 1 - 0.869 = 0.131
$$

Como resultado tenemos:

| M√©todo                | Probabilidad aproximada |
|-----------------------|-------------------------|
| Binomial exacta       | 0.095                   |
| Normal sin correcci√≥n | 0.106                   |
| Normal con correcci√≥n | 0.131                   |

**Interpretaci√≥n:**

\- La **aproximaci√≥n normal** es v√°lida porque se cumplen las condiciones:\
$np = 80 \geq 5$, $n(1 - p) = 20 \geq 5$

\- La **correcci√≥n por continuidad** mejora el ajuste, ya que la binomial es discreta y la normal es continua.

\- Este tipo de aproximaci√≥n permite resolver problemas de forma r√°pida en estudios de calidad, log√≠stica o an√°lisis de eficiencia.

El ejercicio refuerza el papel del **Teorema Central del L√≠mite** como herramienta para pasar de una distribuci√≥n discreta a una continua.

::: note
‚ùì ¬øPor qu√© la correcci√≥n por continuidad se aleja m√°s del valor exacto en este caso?

En el ejemplo de la aproximaci√≥n binomial-normal:

| M√©todo                | Probabilidad aproximada |
|-----------------------|-------------------------|
| Binomial exacta       | 0.095                   |
| Normal sin correcci√≥n | 0.106                   |
| Normal con correcci√≥n | 0.131                   |

Aunque la **correcci√≥n por continuidad** se considera una mejora t√©cnica, **no siempre se traduce en una mejor aproximaci√≥n num√©rica**. En este caso, la normal con correcci√≥n da un valor m√°s lejano al exacto que la normal sin corregir.

üìå ¬øPor qu√© ocurre?

-   La correcci√≥n por continuidad ajusta la normal (continua) a la binomial (discreta) desplazando el umbral:\
    $P(X \geq 85) \rightarrow P(Y \geq 84.5)$
-   Esto **ampl√≠a el √°rea bajo la curva**, aumentando la probabilidad estimada.
-   En los **extremos de la distribuci√≥n** (colas), la binomial y la normal pueden diferir m√°s. La normal tiende a **sobreestimar** las colas.
-   La correcci√≥n mejora el ajuste estructural, pero **puede exagerar** la probabilidad cuando se trata de eventos poco frecuentes.

‚úÖ Conclusi√≥n did√°ctica

> La correcci√≥n por continuidad mejora la adaptaci√≥n entre una distribuci√≥n continua y una discreta, **pero no garantiza un ajuste num√©rico m√°s preciso en todos los casos**.\
> Lo importante es que **respeta mejor la estructura de la binomial**, especialmente cuando se trabaja con rangos o se realizan contrastes.
:::

**Ejercicio 3. Funci√≥n de distribuci√≥n emp√≠rica** $F_n(x)$

Se ha registrado el tiempo (en minutos) que tardan 8 empleados en resolver una tarea:

$$
\{4, 6, 5, 3, 5, 4, 7, 6\}
$$

Se pide:

1.  Calcular la funci√≥n de distribuci√≥n emp√≠rica $F_n(x)$ para esta muestra.
2.  Representar gr√°ficamente $F_n(x)$ (forma escalonada).
3.  Indicar el valor de $F_n(5)$, $F_n(6.5)$, y $F_n(10)$.
4.  Interpretar qu√© significa $F_n(x)$ y c√≥mo se relaciona con la funci√≥n de distribuci√≥n te√≥rica.

*Soluci√≥n:*

Ordenamos los datos:

$$
\{3, 4, 4, 5, 5, 6, 6, 7\}
$$

Recordamos la definici√≥n:

$$
F_n(x) = \frac{\text{n√∫mero de observaciones} \leq x}{n}
$$

Donde $n = 8$. Calculamos $F_n(x)$ para los valores distintos observados:

| $x$ | $F_n(x)$              |
|-----|-----------------------|
| 3   | $\frac{1}{8} = 0.125$ |
| 4   | $\frac{3}{8} = 0.375$ |
| 5   | $\frac{5}{8} = 0.625$ |
| 6   | $\frac{7}{8} = 0.875$ |
| 7   | $\frac{8}{8} = 1.000$ |

Entre valores, la funci√≥n se mantiene constante.

-   $F_n(5) = 0.625$
-   $F_n(6.5) = F_n(6) = 0.875$
-   $F_n(10) = 1$ (todos los valores son menores o iguales)

*Representaci√≥n gr√°fica (escalonada)*

Pueden representarlo como una funci√≥n **escalonada** que **salta** en cada valor observado. El salto en cada punto es de $\frac{1}{8}$, y se mantiene constante entre valores.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Datos del ejercicio
tiempos <- c(4, 6, 5, 3, 5, 4, 7, 6)

# Crear funci√≥n de distribuci√≥n emp√≠rica
Fn <- ecdf(tiempos)

# Secuencia para graficar
x_vals <- seq(2, 8, by = 0.1)

# Graficar la funci√≥n emp√≠rica
plot(Fn, verticals = TRUE, do.points = FALSE,
     main = expression("Funci√≥n de distribuci√≥n emp√≠rica " ~ F[n](x)),
     xlab = "Tiempo (minutos)", ylab = expression(F[n](x)),
     col = "steelblue", lwd = 2)

# A√±adir l√≠neas de referencia
abline(h = seq(0, 1, by = 0.125), v = sort(unique(tiempos)), col = "gray90", lty = 3)

# Mostrar puntos reales
points(sort(tiempos), Fn(sort(tiempos)), col = "steelblue", pch = 16)


```

**Interpretaci√≥n:** La funci√≥n $F_n(x)$ representa la **proporci√≥n acumulada** de observaciones hasta $x$. Es una herramienta muy √∫til para:

-   Visualizar la distribuci√≥n de los datos
-   Comparar con distribuciones te√≥ricas (como la normal o exponencial)
-   Realizar tests de bondad de ajuste (como Kolmog√≥rov-Smirnov)
-   Servir como base para el bootstrap

En este caso, por ejemplo, podemos decir que el **62.5% de los empleados tardaron 5 minutos o menos** en completar la tarea.

**Ejercicio 4. Jerarqu√≠a entre tipos de convergencia**

Se presentan tres sucesiones de variables aleatorias $\{X_n\}$, todas con valor esperado 0 y varianzas decrecientes. Se indica el tipo de convergencia que verifican respecto a una variable aleatoria $X \equiv 0$. En cada caso, responde:

1.  ¬øQu√© tipo de convergencia se verifica?
2.  ¬øQu√© se puede demostrar a partir de ella?
3.  ¬øQu√© otras convergencias no se pueden asegurar?
4.  Justifica tu razonamiento brevemente.

**Casos propuestos**

**a)** La sucesi√≥n $X_n$ verifica que $\mathbb{E}[(X_n - 0)^2] \to 0$

**b)** La sucesi√≥n $X_n$ verifica que $P(|X_n| > \varepsilon) \to 0$ para todo $\varepsilon > 0$

**c)** La sucesi√≥n $X_n$ verifica que $P(\lim_{n \to \infty} X_n = 0) = 1$

*Soluci√≥n guiada*

üîπ Caso a) Se verifica que:

$$
\lim_{n \to \infty} \mathbb{E}[|X_n|^2] = 0
$$

Esto es **convergencia en media cuadr√°tica** hacia 0:\
$$
X_n \xrightarrow{L^2} 0
$$

Demostraci√≥n:

Por definici√≥n, la convergencia en media $r$-√©sima se cumple si:\
$$
\mathbb{E}[|X_n - X|^r] \to 0
$$\
En este caso, con $r = 2$ y $X = 0$, se cumple.

Esto implica:

-   $X_n \xrightarrow{P} 0$ (convergencia en probabilidad)\
-   $X_n \xrightarrow{d} 0$ (convergencia en distribuci√≥n)

y no implica:

-   No se puede concluir que $X_n \to 0$ casi seguramente.\
-   Tampoco garantiza que las trayectorias converjan punto a punto.

üîπ Caso b) Se verifica que:

$$
\forall \varepsilon > 0,\quad P(|X_n| > \varepsilon) \to 0
$$

Esto es la definici√≥n de **convergencia en probabilidad** hacia 0:\
$$
X_n \xrightarrow{P} 0
$$

Demostraci√≥n:

La definici√≥n formal de convergencia en probabilidad se cumple directamente por hip√≥tesis.

Lo qu√© implica:

-   $X_n \xrightarrow{d} 0$ (convergencia en distribuci√≥n)

y no implica:

-   No implica convergencia en media cuadr√°tica: podr√≠an existir varianzas grandes pero con probabilidad concentrada.\
-   No implica convergencia casi segura.

üîπ Caso c) Se verifica que:

$$
P\left( \lim_{n \to \infty} X_n = 0 \right) = 1
$$

Esto es **convergencia casi segura** (tambi√©n llamada con probabilidad 1):\
$$
X_n \xrightarrow{a.s.} 0
$$

Demostraci√≥n:

La hip√≥tesis coincide exactamente con la definici√≥n de convergencia casi segura.

Lo que implica:

-   $X_n \xrightarrow{P} 0$\
-   $X_n \xrightarrow{d} 0$

y no implica:

-   No implica convergencia en media cuadr√°tica (podr√≠an existir valores extremos poco frecuentes que inflen la varianza)

**Conclusi√≥n general**

Este ejercicio ilustra que:

-   La **convergencia casi segura** es la m√°s fuerte.
-   La **convergencia en distribuci√≥n** es la m√°s d√©bil.
-   **Cada tipo de convergencia implica otras m√°s d√©biles**, pero no al rev√©s.

Jerarqu√≠a:

$$
X_n \xrightarrow{a.s.} X \Rightarrow X_n \xrightarrow{P} X \Rightarrow X_n \xrightarrow{d} X
$$ $$
X_n \xrightarrow{L^2} X \Rightarrow X_n \xrightarrow{P} X
$$

## Ejercicio guiado en R

Supongamos que una empresa registra el n√∫mero de unidades vendidas por d√≠a durante una semana:

$$
\text{Ventas} = \{12,\ 9,\ 13,\ 16,\ 8,\ 14,\ 11\}
$$

Queremos:

1.  Estimar la media de ventas por d√≠a.
2.  Estimar la **distribuci√≥n de la media** mediante bootstrap.
3.  Comparar visualmente la distribuci√≥n bootstrap con una distribuci√≥n normal (Teorema Central del L√≠mite).
4.  Reflexionar sobre el uso de aproximaciones en contextos con pocos datos.

**Resoluci√≥n guiada con R**

```{r, message=FALSE, warning=FALSE}
set.seed(2025)

# Datos originales
ventas <- c(12, 9, 13, 16, 8, 14, 11)
n <- length(ventas)

# Estad√≠stico original
media_original <- mean(ventas)
media_original

# Bootstrap
B <- 5000
medias_bootstrap <- replicate(B, mean(sample(ventas, size = n, replace = TRUE)))

# Estimaci√≥n normal seg√∫n TCL
media_hat <- media_original
sd_hat <- sd(ventas) / sqrt(n)

# Gr√°fico
hist(medias_bootstrap, probability = TRUE, col = "skyblue", breaks = 40,
     main = "Distribuci√≥n bootstrap de la media",
     xlab = "Media muestral (bootstrap)", border = "white")

curve(dnorm(x, mean = media_hat, sd = sd_hat),
      add = TRUE, col = "darkred", lwd = 2)

legend("topright", legend = c("Bootstrap", "Normal (TCL)"),
       fill = c("skyblue", NA), border = c("white", NA),
       lty = c(NA, 1), col = c("skyblue", "darkred"), bty = "n")
```

**Interpretaci√≥n:** - El histograma representa la **distribuci√≥n emp√≠rica** de la media de ventas por d√≠a, generada mediante **remuestreo bootstrap** a partir de la muestra original. - La curva representa la **aproximaci√≥n normal** basada en el **Teorema Central del L√≠mite (TCL)**, utilizando la media y desviaci√≥n t√≠pica estimadas de los datos originales. - Aunque la muestra es peque√±a ($n = 7$), la distribuci√≥n bootstrap ya muestra una forma **aproximadamente sim√©trica y unimodal**, pr√≥xima a una distribuci√≥n normal. - Esto sugiere que **la distribuci√≥n de la media muestral converge en distribuci√≥n hacia una normal**, como predice el TCL.

Adem√°s, se observa que:

-   El bootstrap no necesita asumir normalidad de los datos originales.
-   La aproximaci√≥n normal basada en el TCL puede ser **razonable incluso con muestras peque√±as**, aunque con mayor incertidumbre.
