[["probabilidad.html", "Tema 2 Probabilidad 2.1 Introducción 2.2 Sucesos y operaciones con sucesos 2.3 Concepto de probabilidad 2.4 Axiomas de Kolmogórov 2.5 Reglas (Teoremas) de la probabilidad 2.6 Independencia de sucesos 2.7 Combinatoria: Técnicas de enumeración", " Tema 2 Probabilidad 2.1 Introducción A lo largo de nuestra vida —como ciudadanos, analistas o científicos— estamos expuestos constantemente a fenómenos inciertos: los resultados de un experimento, las fluctuaciones de los mercados, el clima, o el comportamiento humano. Todos estos fenómenos comparten una característica común: la incertidumbre. Esta incertidumbre nos lleva a formular preguntas como: - ¿Por qué se ha producido este resultado? - ¿Era previsible? - ¿Cuál es el resultado más probable? - ¿Podría haberse anticipado? - ¿Cómo influye la información disponible en nuestras expectativas? Es aquí donde entra en juego la probabilidad, que proporciona un lenguaje matemático para describir, cuantificar y gestionar la incertidumbre. Nos permite asignar un valor numérico (una probabilidad) a la posibilidad de que ocurra un determinado suceso dentro de un fenómeno aleatorio, , ayudándonos a comprender mejor lo que observamos y, en algunos casos, a anticiparlo. La Estadística, como disciplina, se apoya en dos grandes pilares: Estadística descriptiva: organiza y resume la información observada. Inferencia estadística: extrae conclusiones generales a partir de los datos, basándose en modelos de probabilidad. La probabilidad actúa como puente entre ambas etapas. Antes de poder interpretar datos inciertos o extraer conclusiones válidas, es necesario comprender cómo se comportan los fenómenos aleatorios. Este tema constituye la base para desarrollar ese conocimiento y tiene como objetivo: Introducir los elementos básicos de la teoría de la probabilidad. Analizar distintas formas de entender la probabilidad. Presentar las reglas fundamentales para operar con sucesos. Establecer las bases para trabajar con modelos probabilísticos. Introducir los principios de combinatoria, necesarios para calcular probabilidades en experimentos complejos. A lo largo del tema combinaremos teoría, ejemplos intuitivos y simulaciones con R para que puedas aplicar estos conceptos en problemas reales de análisis de datos, economía o gestión empresarial. 2.2 Sucesos y operaciones con sucesos Antes de estudiar cómo calcular la probabilidad de un fenómeno aleatorio, es fundamental comprender qué es un experimento aleatorio, qué posibles resultados puede tener y cómo representarlos mediante sucesos. 2.2.1 Experimento aleatorio y espacio muestral Un experimento aleatorio es un proceso cuyo resultado no puede preverse con certeza, aunque se conocen todos los posibles resultados que pueden ocurrir. El conjunto de todos los resultados posibles se denomina espacio muestral, y se representa habitualmente por \\(\\Omega\\) o \\(E\\). Ejemplos: Lanzar una moneda una vez: \\(\\Omega = { c, + }\\), donde \\(c\\) representa cara y \\(+\\) cruz. Lanzar una moneda dos veces: \\(\\Omega = { cc, c+, +c, ++ }\\). Lanzar un dado una vez: \\(\\Omega = {1, 2, 3, 4, 5, 6}\\). 2.2.2 Tipos de sucesos Un suceso (también llamado evento) es cualquier subconjunto del espacio muestral: \\(A \\subseteq \\Omega\\). Si contiene un solo resultado → suceso elemental. Si contiene varios → suceso compuesto. Ejemplo: Se lanza un dado y se observa el número que aparece. El espacio muestral es: \\(\\Omega = \\{1, 2, 3, 4, 5, 6\\}\\) - \\(A = \\{2, 4, 6\\}\\): suceso compuesto (“salir número par”). - \\(B = \\{3\\}\\): suceso elemental (“salir un 3”). Casos particulares de de sucesos Suceso seguro: contiene todos los resultados posibles (todos los elementos del espacio muestral). Siempre ocurre. \\(A = \\Omega\\). Figure 2.1: Suceso seguro: A = Ω Suceso imposible: no contiene ningún resultado. Nunca ocurre. \\(A = \\emptyset\\). Sucesos incompatibles: No pueden ocurrir a la vez. \\(A \\cap B = \\emptyset\\). Sucesos compatibles: Pueden ocurrir simultáneamente. \\(A \\cap B \\neq \\emptyset\\). 2.2.3 Operaciones con sucesos Las operaciones con sucesos se corresponden con operaciones de conjuntos. A continuación se presentan las más relevantes, con su interpretación y visualización. Unión: El suceso \\(A \\cup B\\) ocurre si ocurre \\(A\\), \\(B\\), o ambos. Figure 2.2: Unión de sucesos: A ∪ B Intersección: El suceso \\(A \\cap B\\) ocurre solo si ocurren ambos sucesos a la vez. ## Loading required package: futile.logger Complementario: El suceso \\(\\overline{A}\\) (también denotado \\(A^c\\)) ocurre cuando no ocurre el suceso \\(A\\). Diferencia: El suceso \\(A - B\\) ocurre si ocurre \\(A\\) pero no ocurre \\(B\\). Por analogía, el suceso \\(B-A\\) ocurre si ocurre \\(B\\) pero no ocurre \\(A\\). Ejemplo:Dado el espacio muestral \\(\\Omega = \\{1, 2, 3, 4, 5, 6\\}\\), y los sucesos: - \\(A = \\{2, 4, 6\\}\\) - \\(B = \\{1, 2, 3, 4\\}\\) entonces: - \\(A \\cup B = \\{1, 2, 3, 4, 6\\}\\) - \\(A \\cap B = \\{2, 4\\}\\) - \\(\\overline{A} = \\{1, 3, 5\\}\\) - \\(A - B = \\{6\\}\\) 2.2.4 Propiedades del álgebra de sucesos Sean \\(A\\), \\(B\\) y \\(C\\) sucesos del espacio muestral \\(\\Omega\\): Idempotencia: \\(A \\cup A = A\\), \\(A \\cap A = A\\) Conmutativa: \\(A \\cup B = B \\cup A\\), \\(A \\cap B = B \\cap A\\) Asociativa: \\((A \\cup B) \\cup C = A \\cup (B \\cup C)\\), etc. Distributiva: \\((A \\cup B) \\cap C = (A \\cap C) \\cup (B \\cap C)\\) Neutro: \\(A \\cup \\emptyset = A\\), \\(A \\cap \\Omega = A\\) Absorbente: \\(A \\cup \\Omega = \\Omega\\), \\(A \\cap \\emptyset = \\emptyset\\) Complemento: \\(A \\cup \\overline{A} = \\Omega\\), \\(A \\cap \\overline{A} = \\emptyset\\) Doble complemento: \\(\\overline{\\overline{A}} = A\\) Leyes de Morgan: 1ª ley de Morgan: \\(\\overline{A \\cup B} = \\overline{A} \\cap \\overline{B}\\), 2ª Ley de Morgan: \\(\\overline{A \\cap B} = \\overline{A} \\cup \\overline{B}\\) Esta última propiedad es especialmente útil, ya que permite simplificar muchos cálculos. Por ello, vamos a definirla con mayor detalle. Las leyes de De Morgan son reglas fundamentales del álgebra de conjuntos que permiten expresar el complemento de una unión o de una intersección mediante las operaciones inversas. Se formulan del siguiente modo: Primera ley de Morgan: \\(\\overline{A \\cup B} = \\overline{A} \\cap \\overline{B}\\) La primera ley indica que todo lo que no pertenece a la unión de \\(A\\) y \\(B\\), es decir, \\(\\overline{A \\cup B}\\), coincide con lo que está fuera de \\(A\\) y fuera de \\(B\\) simultáneamente. Segunda Ley de Morgan: \\(\\overline{A \\cap B} = \\overline{A} \\cup \\overline{B}\\) La segunda ley señala que todo lo que no pertenece a la intersección de \\(A\\) y \\(B\\), es decir, \\(\\overline{A \\cap B}\\), equivale a lo que no está en \\(A\\) o no está en \\(B\\). Estas leyes nos permiten transformar expresiones con complementos de conjuntos compuestos en otras más manejables, lo cual resulta especialmente útil al calcular probabilidades y trabajar con eventos complejos. ###Ejercicios Dado el espacio muestral \\(\\Omega = \\{1, 2, 3, 4, 5, 6\\}\\), define los siguientes sucesos: Suceso seguro: \\(A = \\Omega\\) Suceso imposible: \\(B = \\emptyset\\) Sucesos incompatibles: \\(C = \\{1, 2\\},\\quad D = \\{3, 4\\}\\) ya que \\(C \\cap D = \\emptyset\\) Sucesos compatibles: \\(E = \\{2, 3, 4\\},\\quad F = \\{4, 5, 6\\}\\) ya que \\(E \\cap F = \\{4\\}\\) Verifica la primera ley de De Morgan para: \\(A = {2, 4, 6}\\),\\(B = {1, 2, 3, 4}\\),\\(\\Omega = {1, 2, 3, 4, 5, 6}\\) \\[ \\overline{A \\cup B} = \\overline{A} \\cap \\overline{B} \\] \\[ A \\cup B = \\{1, 2, 3, 4, 6\\} \\] Calculamos el complementario de \\(A \\cup B\\)** Recordamos que el complemento de un conjunto \\(C\\), denotado \\(\\overline{C}\\), es el conjunto de los elementos de \\(\\Omega\\) que no están en \\(C\\): \\[ \\overline{A \\cup B} = \\Omega - (A \\cup B) = \\{1, 2, 3, 4, 5, 6\\} - \\{1, 2, 3, 4, 6\\} = \\{5\\} \\] Calculamos \\(\\overline{A}\\) y \\(\\overline{B}\\)** \\(\\overline{A} = \\Omega - A = \\{1, 3, 5\\}\\) \\(\\overline{B} = \\Omega - B = \\{5, 6\\}\\) Calculamos la intersección \\(\\overline{A} \\cap \\overline{B}\\)** \\[ \\overline{A} \\cap \\overline{B} = \\{1, 3, 5\\} \\cap \\{5, 6\\} = \\{5\\} \\] Si comparamos los resultados \\(\\overline{A \\cup B} = \\{5\\}\\) \\(\\overline{A} \\cap \\overline{B} = \\{5\\}\\) ambos conjuntos coinciden, por tanto: \\[ \\boxed{\\overline{A \\cup B} = \\overline{A} \\cap \\overline{B}} \\] 2.3 Concepto de probabilidad La probabilidad es el lenguaje matemático con el que medimos la incertidumbre. Nos permite asignar un valor numérico a qué tan probable es que ocurra un determinado suceso dentro de un experimento aleatorio. Esta asignación es clave cuando pasamos de describir lo que ha ocurrido —estadística descriptiva— a estimar lo que podría ocurrir en el futuro o en otros contextos —inferencia estadística—. En estadística, trabajamos habitualmente con muestras para extraer conclusiones sobre poblaciones. Sin embargo, como las muestras se obtienen al azar, necesitamos herramientas para cuantificar hasta qué punto las conclusiones que obtenemos son fiables. Aquí es donde entra en juego la probabilidad: es la base teórica que justifica los métodos inferenciales. El objetivo de esta sección es proporcionar una primera aproximación al concepto de probabilidad, mostrar las principales interpretaciones existentes y presentar las herramientas que nos permitirán calcularla en distintos contextos. Estos fundamentos son esenciales para todo el análisis probabilístico posterior, incluyendo el estudio de variables aleatorias y distribuciones. La probabilidad es una herramienta matemática fundamental para analizar fenómenos inciertos. Su utilidad es clave en la toma de decisiones bajo incertidumbre, en ámbitos tan diversos como los seguros, la economía, la gestión empresarial o las ciencias sociales. Antes de aplicar reglas o fórmulas, es necesario entender qué significa hablar de la probabilidad de un suceso. 2.3.1 Interpretaciones de la probabilidad 2.3.1.1 Probabilidad clásica Una de las primeras formas de entender la probabilidad es la llamada probabilidad clásica, también conocida como probabilidad a priori. Esta interpretación surge en el contexto de los juegos de azar, donde todos los resultados posibles se consideran igualmente probables. Definición: La probabilidad clásica de un suceso \\(A\\) se define como el cociente entre el número de casos favorables a \\(A\\) y el número total de casos posibles, siempre que todos ellos tengan la misma probabilidad de ocurrir: \\[ P(A) = \\frac{\\text{número de casos favorables a } A}{\\text{número total de casos posibles}} = \\frac{n(A)}{n(\\Omega)} \\] donde: \\(n(A)\\): número de resultados favorables al suceso \\(A\\) \\(n(\\Omega)\\): número total de resultados posibles en el espacio muestral \\(\\Omega\\) Condiciones de aplicación Esta definición solo es válida cuando: El espacio muestral es finito Todos los resultados elementales son equiprobables 2.3.1.1.1 Propiedades de la probabilidad clásica A partir de esta definición, se pueden deducir las siguientes propiedades fundamentales: Rango: La probabilidad de cualquier suceso está entre 0 y 1: \\[ 0 \\leq P(A) \\leq 1 \\] Suceso imposible: \\[ P(\\emptyset) = 0 \\] Suceso seguro: \\[ P(\\Omega) = 1 \\] Complementario: \\[ P(\\overline{A}) = 1 - P(A) \\] Adición para sucesos incompatibles: \\[ A \\cap B = \\emptyset \\Rightarrow P(A \\cup B) = P(A) + P(B) \\] Monotonía: \\[ A \\subseteq B \\Rightarrow P(A) \\leq P(B) \\] Ejemplo Al lanzar un dado justo, la probabilidad de obtener un número par es: \\[ P(\\text{par}) = \\frac{3}{6} = 0{,}5 \\] 2.3.1.2 Probabilidad frecuentista Una interpretación alternativa y muy influyente de la probabilidad es la frecuentista. Esta concepción se basa en la observación de fenómenos repetidos un gran número de veces. Definición: La probabilidad frecuentista de un suceso \\(A\\) se define como el límite de la frecuencia relativa de aparición de \\(A\\) cuando el experimento se repite muchas veces en condiciones similares: \\[ P(A) = \\lim_{n \\to \\infty} \\frac{n(A)}{n} \\] donde: \\(n(A)\\): número de veces que ocurre el suceso \\(A\\) \\(n\\): número total de repeticiones del experimento Esta definición se entiende como una propiedad empírica del experimento aleatorio: cuanto mayor sea el número de repeticiones, más se estabiliza la frecuencia relativa del suceso \\(A\\). Condiciones de aplicación El experimento debe poder repetirse en condiciones similares o idénticas. Se requiere un número suficientemente grande de repeticiones para aproximarse al valor de la probabilidad. 2.3.1.2.1 Propiedades de la probabilidad frecuentista Al igual que con la probabilidad clásica, se cumplen las propiedades fundamentales: \\(0 \\leq P(A) \\leq 1\\) \\(P(\\Omega) = 1\\) \\(P(\\emptyset) = 0\\) \\(P(\\overline{A}) = 1 - P(A)\\) Si \\(A \\cap B = \\emptyset\\), entonces \\(P(A \\cup B) = P(A) + P(B)\\) Estas propiedades pueden observarse empíricamente al analizar datos de experimentos reales repetidos muchas veces. Ejemplo Supongamos que lanzamos una moneda 1.000 veces y obtenemos 513 caras. La probabilidad frecuentista de obtener cara se estima como: \\[ P(\\text{cara}) \\approx \\frac{513}{1000} = 0.513 \\] A medida que se incrementa el número de lanzamientos, esta frecuencia relativa tiende a estabilizarse en torno al valor teórico (0,5 si la moneda es equilibrada). 2.3.1.3 Probabilidad bayesiana La probabilidad bayesiana interpreta la probabilidad como un grado de creencia subjetivo que tiene una persona sobre la ocurrencia de un suceso, en función de la información disponible. Este enfoque reconoce que las personas pueden asignar probabilidades diferentes a un mismo suceso, dependiendo del conocimiento previo que posean. A diferencia de la probabilidad clásica o frecuentista, no exige que el experimento sea repetible ni que todos los resultados sean equiprobables. Características clave Subjetiva: se basa en el conocimiento, experiencia o información previa del observador. Dinamismo: la probabilidad se actualiza cuando se dispone de nueva información (usando el Teorema de Bayes). Útil en contextos de incertidumbre parcial, como la medicina, el diagnóstico, las decisiones económicas, etc. Ejemplo Supongamos que sabemos que un paciente pertenece a un grupo de riesgo de cierta enfermedad. Esto nos lleva a asignar una probabilidad inicial (o a priori) de que esté enfermo. Tras realizarle una prueba médica, si el resultado es positivo, actualizamos nuestra creencia sobre su estado de salud combinando la información previa con la nueva evidencia. Este proceso se realiza aplicando el Teorema de Bayes, que permite pasar de la probabilidad a priori a una probabilidad a posteriori. Interpretación La probabilidad bayesiana no es una propiedad objetiva del suceso, sino una expresión del conocimiento y la incertidumbre del observador. Por ello, es especialmente útil en situaciones donde la información es incompleta o se va obteniendo progresivamente. 2.4 Axiomas de Kolmogórov La formulación matemática moderna de la probabilidad se basa en el sistema axiomático propuesto por Andréi Kolmogórov en 1933. Este enfoque establece las reglas fundamentales que debe cumplir toda asignación de probabilidades. Sean \\(\\Omega\\) el espacio muestral y \\(\\mathcal{F}\\) una colección de sucesos (subconjuntos de \\(\\Omega\\)). Una función \\(P\\) que asigna un número real \\(P(A)\\) a cada suceso \\(A \\in \\mathcal{F}\\) es una probabilidad si cumple los siguientes axiomas: 2.4.1 Axioma 1: No negatividad \\[ \\forall A \\in \\mathcal{F}, \\quad P(A) \\geq 0 \\] La probabilidad de cualquier suceso es un número mayor o igual que cero. 2.4.2 Axioma 2: Normalización \\[ P(\\Omega) = 1 \\] La probabilidad del suceso seguro (es decir, el conjunto de todos los posibles resultados) es igual a 1. ###Axioma 3: Aditividad numerable Si \\(A_1, A_2, A_3, \\ldots\\) son sucesos mutuamente incompatibles (es decir, \\(A_i \\cap A_j = \\emptyset\\) para \\(i \\neq j\\)), entonces: \\[ P\\left( \\bigcup_{i=1}^{\\infty} A_i \\right) = \\sum_{i=1}^{\\infty} P(A_i) \\] Este axioma se conoce como sigma-aditividad y garantiza que la probabilidad se comporta de forma coherente incluso cuando consideramos una sucesión infinita de sucesos disjuntos. La tripleta \\((\\Omega, \\mathcal{F}, P)\\) se denomina espacio de probabilidad. Justificación y contexto Aunque estos axiomas se aceptan sin demostración, tienen una fuerte motivación basada en la realidad que modelan: Los dos primeros axiomas pueden justificarse desde la teoría clásica (equiprobabilidad) y la frecuentista (frecuencias relativas): La probabilidad no debe ser negativa. La probabilidad del suceso seguro debe ser 1. El tercer axioma es más técnico: en la práctica, se trabaja con sucesiones finitas de sucesos, pero su generalización a casos infinitos es fundamental en el plano teórico y en el desarrollo de la probabilidad continua. 2.4.3 Propiedades derivadas de los axiomas A partir de los tres axiomas de Kolmogórov, se pueden deducir una serie de propiedades que facilitan el cálculo de probabilidades y permiten desarrollar el resto de la teoría: 1. Probabilidad del suceso imposible El suceso imposible es el conjunto vacío, \\(\\emptyset\\), y su probabilidad es: \\[ P(\\emptyset) = 0 \\] Esto se deduce del axioma 3 aplicándolo a una familia vacía de sucesos. 2. Probabilidad del Complemento de un suceso Dado un suceso \\(A\\), su complemento se denota \\(\\overline{A}\\) (o \\(A^c\\)) y representa que no ocurre \\(A\\). Entonces: \\[ P(\\overline{A}) = 1 - P(A) \\] Esto se deduce usando que \\(A \\cup \\overline{A} = \\Omega\\) y que \\(A \\cap \\overline{A} = \\emptyset\\). 3. Monotonía Si un suceso \\(A\\) está contenido en otro suceso \\(B\\), es decir \\(A \\subseteq B\\), entonces: \\[ P(A) \\leq P(B) \\] 4. Aditividad finita Si \\(A\\) y \\(B\\) son sucesos incompatibles, es decir \\(A \\cap B = \\emptyset\\), entonces: \\[ P(A \\cup B) = P(A) + P(B) \\] Este resultado es un caso particular del axioma 3. Fórmula de la unión para dos sucesos Cuando \\(A\\) y \\(B\\) no son disjuntos, se puede calcular la probabilidad de su unión mediante: \\[ P(A \\cup B) = P(A) + P(B) - P(A \\cap B) \\] 5.Probabilidad de la diferencia de sucesos La probabilidad de \\(A - B\\) (es decir, de que ocurra \\(A\\) pero no \\(B\\)) es: \\[ P(A - B) = P(A) - P(A \\cap B) \\] Todas estas propiedades se aplican de forma sistemática en los ejercicios prácticos y en el desarrollo de técnicas más avanzadas, como el cálculo de probabilidades condicionadas o el uso del teorema de Bayes. 2.5 Reglas (Teoremas) de la probabilidad Una vez comprendido el concepto de probabilidad, es necesario aprender a combinarla en distintos contextos. Las reglas que veremos a continuación nos permiten calcular probabilidades más complejas, muchas veces necesarias en aplicaciones estadísticas, financieras o de gestión de riesgos. Estas reglas permiten calcular probabilidades más complejas combinando sucesos conocidos. Son fundamentales en aplicaciones como la predicción de riesgos o el diagnóstico médico. A partir de los axiomas de Kolmogórov y las propiedades deducidas, se pueden establecer una serie de resultados fundamentales que permiten calcular probabilidades en situaciones más complejas. A continuación presentamos los más importantes. 2.5.1 Probabilidad condicionada La probabilidad condicionada de un suceso \\(A\\) dado que ha ocurrido otro suceso \\(B\\) (con \\(P(B) &gt; 0\\)) se define como: \\[ P(A \\mid B) = \\frac{P(A \\cap B)}{P(B)} \\] Esta expresión representa la probabilidad de que ocurra \\(A\\), sabiendo que ya ha ocurrido \\(B\\). Es útil cuando tenemos información adicional que modifica nuestro punto de vista sobre el experimento. Ejemplo: Si sabemos que ha salido un número par al lanzar un dado, la probabilidad de que sea mayor que 3 ya no es \\(\\frac{3}{6}\\), sino \\(\\frac{2}{3}\\), porque solo consideramos los pares: \\(\\{2,4,6\\}\\). 2.5.2 Regla del producto A partir de la definición de probabilidad condicionada, se obtiene la regla del producto para dos sucesos: \\[ P(A \\cap B) = P(B) \\cdot P(A \\mid B) = P(A) \\cdot P(B \\mid A) \\] Esta fórmula permite calcular la probabilidad de la intersección de dos sucesos. 2.5.3 Teorema de la probabilidad total Sea \\(\\{B_1, B_2, \\dots, B_n\\}\\) una partición del espacio muestral \\(\\Omega\\) (es decir, sucesos incompatibles y cuya unión es \\(\\Omega\\)), y sea \\(A\\) un suceso cualquiera. Entonces: \\[ P(A) = \\sum_{i=1}^n P(B_i) \\cdot P(A \\mid B_i) \\] Este teorema permite descomponer la probabilidad de un suceso complejo en términos de probabilidades condicionadas, lo que resulta especialmente útil cuando se tienen diferentes escenarios posibles. 2.5.4 Teorema de Bayes Este resultado permite invertir la condición en una probabilidad condicionada. Es decir, calcular \\(P(B_i \\mid A)\\) a partir de \\(P(A \\mid B_i)\\) y \\(P(B_i)\\). \\[ P(B_i \\mid A) = \\frac{P(B_i) \\cdot P(A \\mid B_i)}{\\sum_{j=1}^n P(B_j) \\cdot P(A \\mid B_j)} \\] Este teorema se entiende mejor visualmente a través de un diagrama de árbol, que representa cómo se ramifican los sucesos condicionales. Ejemplo resuelto con el Teorema de Bayes Supongamos que en una población: El 40% fuma y el 60% no fuma. Entre los fumadores, el 75% son hombres. Entre los no fumadores, el 40% son hombres. Figure 2.3: Diagrama de árbol para el Teorema de Bayes Pregunta: Si una persona seleccionada al azar es hombre, ¿cuál es la probabilidad de que fume? ✅ Paso 1: Definir los sucesoss - \\(F\\): la persona fuma - \\(\\overline{F}\\): la persona no fuma - \\(H\\): la persona es hombre Queremos calcular: \\[ P(F \\mid H) = ? \\] ✅ Paso 2: Aplicar el Teorema de Bayes \\[ P(F \\mid H) = \\frac{P(F) \\cdot P(H \\mid F)}{P(F) \\cdot P(H \\mid F) + P(\\overline{F}) \\cdot P(H \\mid \\overline{F})} \\] Sustituimos los datos del árbol: \\[ P(F \\mid H) = \\frac{0.4 \\cdot 0.75}{0.4 \\cdot 0.75 + 0.6 \\cdot 0.4} = \\frac{0.3}{0.3 + 0.24} = \\frac{0.3}{0.54} \\approx 0{,}556 \\] ✅ Conclusión La probabilidad de que una persona fume, sabiendo que es hombre, es aproximadamente 0,556. Este ejemplo muestra cómo el Teorema de Bayes nos permite invertir la condición y obtener una probabilidad a posteriori a partir de datos previos y condicionales. Permite invertir probabilidades condicionadas, pasando de \\(P(A \\mid B)\\) a \\(P(B \\mid A)\\). Es la base del razonamiento bayesiano: \\[ P(B_j \\mid A) = \\frac{P(A \\mid B_j) P(B_j)}{\\sum_{i=1}^n P(A \\mid B_i) P(B_i)} \\] Este teorema es la base de la probabilidad bayesiana, y se aplica frecuentemente en diagnóstico médico, toma de decisiones y clasificación en ciencia de datos. Ejemplo típico: Si un test médico da positivo, ¿cuál es la probabilidad de que realmente el paciente esté enfermo? Bayes permite responder a esta pregunta teniendo en cuenta la probabilidad previa de enfermedad y las tasas de falsos positivos y negativos. 2.5.4.1 Ejercicios Teorema de Bayes 2.5.4.1.1 Ejercicio 1: Diagnóstico médico Una enfermedad afecta al 2 % de la población. Existe una prueba médica que: Da positivo en el 99 % de los casos si la persona está enferma. Da positivo en el 5 % de los casos si la persona no está enferma (falso positivo). Pregunta: Si una persona da positivo en la prueba, ¿cuál es la probabilidad de que esté realmente enferma? ✅Solución Paso 1: Definir los sucesos \\(E\\): persona está enferma \\(\\overline{E}\\): persona no está enferma \\(+\\): la prueba da positivo Queremos calcular: \\[ P(E \\mid +) \\] Paso 2: Aplicar el Teorema de Bayes \\[ P(E \\mid +) = \\frac{P(E) \\cdot P(+ \\mid E)}{P(E) \\cdot P(+ \\mid E) + P(\\overline{E}) \\cdot P(+ \\mid \\overline{E})} \\] Sustituimos: [ P(E +) = = = ] Conclusión: Aunque la prueba sea bastante fiable, la probabilidad de que una persona realmente esté enferma si da positivo es solo 0,288. Esto se debe a que la enfermedad es poco frecuente. 2.5.4.1.2 Ejercicio 2: Contratación en una empresa Una empresa contrata a candidatos de dos universidades: El 30% de los contratados proviene de la Universidad A, y el 70% de la Universidad B. El 90% de los egresados de A superan el periodo de prueba. El 60% de los egresados de B lo superan. Pregunta: Si un empleado ha superado el periodo de prueba, ¿cuál es la probabilidad de que haya estudiado en la Universidad A? ✅ Solución guiada Paso 1: Definir los sucesos \\(A\\): proviene de la Universidad A \\(B\\): proviene de la Universidad B \\(S\\): supera el periodo de prueba Queremos calcular: \\[ P(A \\mid S) \\] Paso 2: Aplicar el Teorema de Bayes \\[ P(A \\mid S) = \\frac{P(A) \\cdot P(S \\mid A)}{P(A) \\cdot P(S \\mid A) + P(B) \\cdot P(S \\mid B)} \\] Sustituimos: \\[ P(A \\mid S) = \\frac{0.3 \\cdot 0.9}{0.3 \\cdot 0.9 + 0.7 \\cdot 0.6} = \\frac{0.27}{0.27 + 0.42} = \\frac{0.27}{0.69} \\approx 0.391 \\] Conclusión: Si un empleado supera el periodo de prueba, la probabilidad de que provenga de la Universidad A es aproximadamente 0.391. 2.5.4.2 Ejercicio con R: Clasificación de correos como spam (aprendizaje automático) En un sistema de detección de spam, se ha entrenado un clasificador que detecta si un correo es spam o no, basado en ciertas palabras clave. Se sabe que: El 10% de los correos recibidos son spam. El clasificador: Detecta correctamente un spam el 98 % de las veces (sensibilidad). Clasifica como no spam correctamente un 90 % de los correos legítimos (especificidad). Pregunta: Si un correo ha sido marcado como spam, ¿cuál es la probabilidad de que realmente lo sea? Solución En primer lugar simulamos con R: set.seed(42) # Tamaño de muestra n &lt;- 10000 # Generamos si el correo es spam (10%) es_spam &lt;- rbinom(n, 1, 0.10) # Clasificador identifica spam con sensibilidad 98% y especificidad 90% marcado_spam &lt;- ifelse(es_spam == 1, rbinom(n, 1, 0.98), # verdadero positivo rbinom(n, 1, 0.10)) # falso positivo # Guardamos en un data.frame correos &lt;- data.frame(es_spam, marcado_spam) # Tabla de frecuencias table(correos$marcado_spam, correos$es_spam, dnn = c(&quot;Marcado como spam&quot;, &quot;Es spam&quot;)) ## Es spam ## Marcado como spam 0 1 ## 0 8093 20 ## 1 876 1011 A continuación calculamos la probabilidad empírica: # Correos marcados como spam correos_marcados &lt;- correos[correos$marcado_spam == 1, ] # Proporción de verdaderos spam entre los marcados prob_spam_dado_marcado &lt;- mean(correos_marcados$es_spam) prob_spam_dado_marcado ## [1] 0.5357711 Aunque el clasificador es bastante preciso (98% de sensibilidad y 90% de especificidad), solo el 53.6% de los correos marcados como spam son realmente spam. Esto ocurre porque el evento “ser spam” es poco frecuente (solo el 10% de todos los correos). En estos casos, incluso una pequeña tasa de error puede provocar que muchos correos legítimos sean marcados erróneamente como spam (falsos positivos), y eso hace que la probabilidad de que un correo marcado como spam sea realmente spam no sea tan alta como cabría esperar. Conclusión clave: El valor del 53.6% muestra que en contextos con baja prevalencia (como el spam), la probabilidad posterior puede alejarse bastante de la sensibilidad del test. Es decir, tener un buen clasificador no garantiza una alta fiabilidad en los positivos si el evento que se intenta detectar es raro. Esto es una consecuencia directa del teorema de Bayes. Visualización Interpretación del gráfico Barra izquierda (Spam): Mayoritariamente clasificados correctamente como spam (color rojo). Barra derecha (No spam): Un pequeño porcentaje se clasifica erróneamente como spam (falsos positivos). ✅ Conclusión Con esta simulación, hemos estimado empíricamente la probabilidad \\(P(Spam∣Marcado)\\), es decir, la probabilidad de que un correo realmente sea spam si ha sido clasificado como tal. Este valor estará alrededor del 52%, mostrando cómo incluso un buen clasificador puede producir una alta tasa de falsos positivos cuando el evento es poco frecuente. 2.6 Independencia de sucesos En muchos fenómenos reales, algunos sucesos no se afectan entre sí. Por ejemplo, lanzar un dado y tirar una moneda son experimentos que no interfieren uno con otro. El concepto de independencia formaliza esta idea y nos permite simplificar notablemente los cálculos cuando se cumple. Es especialmente relevante en modelos de probabilidad compuesta y en teoría estadística. La independencia permite identificar situaciones en las que un suceso no afecta a la ocurrencia de otro, algo muy relevante para modelos de riesgo o predicciones múltiples. Definición Dos sucesos \\(A\\) y \\(B\\) se consideran independientes cuando la ocurrencia de uno de ellos (por ejemplo, \\(B\\)) no modifica la probabilidad de que ocurra el otro (\\(A\\)). Matemáticamente, esta relación se expresa mediante las igualdades: \\(P(A \\mid B) = P(A)\\) \\(P(B \\mid A) = P(B)\\) Es decir, si \\(A\\) y \\(B\\) son independientes, conocer que ha ocurrido uno de ellos no aporta información adicional sobre la ocurrencia del otro. La probabilidad de \\(A\\) condicionada a \\(B\\) coincide con la probabilidad incondicional de \\(A\\), y lo mismo sucede con \\(B\\) condicionado a \\(A\\). Por tanto, si dos sucesos \\(A\\) y \\(B\\) son independientes se verifica que: \\[ P(A \\cap B) = P(A) P(B) \\] En estas circunastancias: \\(P(A \\mid B) = \\frac{P(A \\cap B)}{P(A)}=P(A)\\) Ejemplo: Lanzar una moneda y tirar un dado. El resultado del dado no influye en la moneda. ### Ejemplo: Independencia entre dos sucesos Supongamos que lanzamos un dado y tiramos una moneda. Definimos los siguientes sucesos: \\(A\\): obtener un número par en el dado. \\(B\\): obtener cara en la moneda. Queremos saber si los sucesos \\(A\\) y \\(B\\) son independientes. Calculamos \\(P(A)\\) En un dado hay 3 números pares: 2, 4 y 6. Por tanto: \\[ P(A) = \\frac{3}{6} = 0.5 \\] Calculamos \\(P(B)\\) La moneda tiene dos caras: cara y cruz. La probabilidad de sacar cara es: \\[ P(B) = \\frac{1}{2} = 0.5 \\] Calculamos \\(P(A \\cap B)\\) Como el resultado del dado no afecta al de la moneda y viceversa, el total de posibles combinaciones es \\(6 \\times 2 = 12\\). Las combinaciones que dan par y cara son: (2, cara), (4, cara), (6, cara) → 3 casos favorables. \\[ P(A \\cap B) = \\frac{3}{12} = 0.25 \\] Verificamos si \\(P(A \\cap B) = P(A) \\cdot P(B)\\) \\[ P(A) \\cdot P(B) = 0.5 \\cdot 0.5 = 0.25 = P(A \\cap B) \\] Como se cumple la igualdad: \\[ \\boxed{A \\text{ y } B \\text{ son sucesos independientes}} \\] Simulación en R Vamos a simular este experimento 100.000 veces y comprobar empíricamente la independencia: set.seed(42) n &lt;- 100000 dado &lt;- sample(1:6, n, replace = TRUE) moneda &lt;- sample(c(&quot;cara&quot;, &quot;cruz&quot;), n, replace = TRUE) A &lt;- dado %% 2 == 0 B &lt;- moneda == &quot;cara&quot; p_A &lt;- mean(A) p_B &lt;- mean(B) p_AB &lt;- mean(A &amp; B) c(P_A = p_A, P_B = p_B, P_AyB = p_AB, Producto = p_A * p_B, dif = p_AB - p_A * p_B) ## P_A P_B P_AyB Producto dif ## 0.498520000 0.498450000 0.249800000 0.248487294 0.001312706 Nota: al ser un experimento empírico, la igualdad no se cumple perfectamente. Obsérvese que sacar un número par en un lanzamiento de un dado no es exáctamente 0.5, y lo mismo ocurre con el número de caras al lanzar una moneda. Si se incrementa el número de repeticiones se espera que esas diferencias son menores. 2.6.1 Regla del producto o de la multiplicación La regla del producto o multiplicación permite calcular la probabilidad de que ocurran simultáneamente varios sucesos, incluso si no son independientes. Para \\(n\\) sucesos \\(A_1, A_2, \\dots, A_n\\), la probabilidad conjunta se puede expresar como una cadena de probabilidades condicionadas: \\[ P(A_1 \\cap A_2 \\cap \\dots \\cap A_n) = P(A_1) \\cdot P(A_2 \\mid A_1) \\cdot P(A_3 \\mid A_1 \\cap A_2) \\cdot \\dots \\cdot P(A_n \\mid A_1 \\cap A_2 \\cap \\dots \\cap A_{n-1}) \\] Esta fórmula nos dice que para conocer la probabilidad de que ocurran todos los sucesos a la vez, primero calculamos la probabilidad del primero, luego la del segundo en función de que haya ocurrido el primero, después la del tercero dado que han ocurrido los dos anteriores, y así sucesivamente. Cuando los sucesos son independientes entre sí, esta fórmula se simplifica notablemente, ya que las probabilidades condicionadas se igualan a las incondicionales. En ese caso, basta con multiplicar las probabilidades individuales: \\[ P(A_1 \\cap A_2 \\cap \\dots \\cap A_n) = P(A_1) \\cdot P(A_2) \\cdot \\dots \\cdot P(A_n) \\] Este resultado es una consecuencia directa de la definición de independencia para más de dos sucesos. Ejemplo: Aplicación de la regla del producto Supongamos que en una empresa se están seleccionando candidatos para tres puestos diferentes: A, B y C. Cada selección depende de la anterior: La probabilidad de seleccionar un candidato adecuado para el puesto A es \\(P(A) = 0.9\\). Si se ha seleccionado a alguien para A, la probabilidad de encontrar un candidato adecuado para B es \\(P(B \\mid A) = 0.8\\). Si se han seleccionado candidatos para A y B, la probabilidad de seleccionar a uno adecuado para C es \\(P(C \\mid A \\cap B) = 0.7\\). Queremos calcular la probabilidad de que se seleccionen buenos candidatos para los tres puestos a la vez, es decir: \\[ P(A \\cap B \\cap C) \\] En primer lugar, según la regla del producto: \\[ P(A \\cap B \\cap C) = P(A) \\cdot P(B \\mid A) \\cdot P(C \\mid A \\cap B) \\] Si sustituimos valores \\[ P(A \\cap B \\cap C) = 0.9 \\cdot 0.8 \\cdot 0.7=0.504 \\] Por tanto, la probabilidad de que se encuentren buenos candidatos para los tres puestos es del 50.4%: \\[ \\boxed{P(A \\cap B \\cap C) = 0.504} \\] Podemos confirmar este resultado mediante simulación. Supongamos que repetimos este proceso 10000 veces: set.seed(123) n &lt;- 10000 A &lt;- runif(n) &lt; 0.9 B &lt;- runif(n) &lt; 0.8 &amp; A # B depende de A C &lt;- runif(n) &lt; 0.7 &amp; A &amp; B # C depende de A y B # Probabilidad empírica de que A, B y C ocurran a la vez mean(A &amp; B &amp; C) ## [1] 0.5041 2.7 Combinatoria: Técnicas de enumeración Para aplicar la probabilidad clásica o calcular el tamaño de espacios muestrales, a menudo necesitamos contar cuántas formas hay de que ocurran determinados sucesos. La combinatoria o técnicas de enumeración (principio de multiplicación, permutaciones y combinaciones) nos proporcionan métodos sistemáticos para hacerlo. Son herramientas clave cuando trabajamos con espacios discretos finitos. Estas herramientas permiten contar de forma eficiente el número de casos posibles, crucial para calcular probabilidades en espacios muestrales grandes o abstractos. 2.7.1 Principio de multiplicación Si un experimento consta de \\(k\\) etapas independientes, y cada etapa puede realizarse de un número determinado de formas (\\(n_i\\)), el número total de formas de realizar el experimento completo es el producto del número de opciones en cada etapa: \\[ \\text{Total de casos} = n_1 \\cdot n_2 \\cdot \\cdots \\cdot n_k \\] Condiciones: Las elecciones son independientes entre sí. Se realiza una acción por cada etapa. Ejemplo: Un menú tiene 2 primeros platos, 3 segundos y 2 postres. ¿Cuántos menús diferentes pueden hacerse? \\(2 \\times 3 \\times 2 = 12 \\text{ menús distintos}\\) 2.7.2 Combinaciones Las combinaciones se utilizan cuando lo que importa es el conjunto elegido, no el orden. Es decir, se trata de selecciones sin importar cómo se ordenan. Esto es común en contextos como la formación de comités, grupos de trabajo o en la elección de cartas en un juego. 2.7.2.1 Combinaciones sin repetición Una combinación sin repetición es una selección de \\(k\\) elementos de un conjunto de \\(n\\) elementos, sin repetición y sin importar el orden. \\[ C(n,k) = \\binom{n}{k} = \\frac{n!}{k!(n-k)!} \\] Se utiliza cuando se quiere saber de cuántas formas se pueden elegir \\(k\\) elementos de entre \\(n\\), sin repetir y sin importar el orden. Ejemplo: ¿Cuántos grupos de 3 estudiantes se pueden formar a partir de un grupo de 5? \\[ C(5,3) = \\frac{5!}{3! \\cdot 2!} = 10 \\] En R choose(5, 3) ## [1] 10 2.7.2.2 Combinaciones con repetición: Una combinación con repetición es una forma de seleccionar \\(k\\) elementos de un conjunto de \\(n\\) elementos, permitiendo la repetición de elementos y sin importar el orden en que se seleccionan. \\[ C&#39;(n,k) = \\binom{n + k - 1}{k} = \\frac{(n + k - 1)!}{k!(n - 1)!} \\] Se utiliza cuando queremos saber cuántas maneras hay de elegir \\(k\\) elementos de un conjunto de \\(n\\) opciones con repetición permitida y donde no importa el orden. Ejemplo Supongamos que disponemos de 4 tipos de caramelos y queremos escoger 3 (permitiendo repetir sabores). ¿Cuántas combinaciones diferentes de caramelos podemos formar? \\[ C&#39;(4,3) = \\binom{4 + 3 - 1}{3} = \\binom{6}{3} = 20 \\] En R # Número de combinaciones con repetición de 3 elementos tomados de 4 tipos choose(4 + 3 - 1, 3) ## [1] 20 2.7.3 Variaciones Las variaciones son disposiciones de elementos tomados de un conjunto, en las que importa el orden. Pueden ser con o sin repetición, según si se permite o no repetir elementos en la selección. 2.7.3.1 Variaciones sin repetición Una variación sin repetición es una forma de seleccionar y ordenar \\(k\\) elementos de un conjunto de \\(n\\) elementos, sin repetir elementos. La fórmula de cálculo es: \\[ V(n,k) = \\frac{n!}{(n-k)!} \\] Las variaciones se usan cuando se desea contar cuántas formas hay de ordenar \\(k\\) elementos tomados de un total de \\(n\\), sin repetir ningún elemento. Ejemplo práctico ¿Cuántos números de 2 cifras distintas se pueden formar con los dígitos del 1 al 5? \\[ V(5,2) = \\frac{5!}{(5 - 2)!} = \\frac{120}{6} = 20 \\] Código en R No existe una fórmula directa para calcular las variaciones, así que se calcula directamente mediante factoriales. # Variaciones sin repetición: V(5,2) factorial(5) / factorial(5 - 2) ## [1] 20 ####Variaciones con repetición Una variación con repetición es una disposición de \\(K\\) elementos seleccionados de un conjunto de \\(n\\) elementos, permitiendo repetición y teniendo en cuenta el orden. SU fórmula es: \\[ V&#39;(n,k) = n^k \\] Las variaciones con repetición se utilizan cuando se desea contar cuántas formas hay de ordenar \\(k\\) elementos de entre \\(n\\), con repetición y cuando el orden importa. Ejemplo práctico ¿Cuántos códigos de 3 dígitos se pueden formar con los números del 1 al 4 si se pueden repetir? \\[ V&#39;(4,3) = 4^3 = 64 \\] Código en R # Variaciones con repetición: V&#39;(4,3) 4^3 ## [1] 64 2.7.4 Permutaciones Las permutaciones son disposiciones u ordenaciones de elementos. Se caracterizan porque el orden sí importa. Se utilizan cuando queremos contar de cuántas maneras diferentes se pueden ordenar o disponer elementos, y el orden importa. Son muy comunes en contextos donde cada posición tiene un significado distinto, como la asignación de premios o el orden de llegada en una competición. Se dividen en dos tipos principales: sin repetición y con repetición. 2.7.4.1 Permutaciones sin repetición Una permutación sin repetición es una ordenación de todos los elementos de un conjunto de \\(n\\) elementos, sin repetir ninguno. Su fórmula es: \\[ P(n) = n! \\] Se emplean cuando queremos contar el número de formas diferentes de ordenar todos los elementos de un conjunto, sin que se repita ninguno. Ejemplo práctico ¿Cuántas formas hay de ordenar las letras de la palabra ROMA? Como tiene 4 letras distintas: \\[ P(4) = 4! = 24 \\] Código en R No hay una fórmula expresa para las permuntaciones, sino que se caclula con la función ‘factorial’. # Permutaciones sin repetición: P(4) factorial(4) ## [1] 24 2.7.4.2 Permutaciones con repetición Una permutación con repetición es una ordenación de un conjunto de elementos donde algunos son idénticos entre sí. Se trata de contar cuántas formas distintas hay de ordenar dichos elementos, sin distinguir los repetidos. \\[ P(n; n_1, n_2, \\dots, n_k) = \\frac{n!}{n_1! \\cdot n_2! \\cdots n_k!} \\] Donde: - \\(n\\) es el número total de elementos, - \\(n_1, n_2, \\dots, n_k\\) representan las repeticiones de cada tipo de elemento indistinguible. Se usa cuando hay elementos repetidos en el conjunto, y queremos contar las disposiciones distintas teniendo en cuenta que los elementos iguales no se diferencian. Ejemplo práctico ¿Cuántas formas distintas hay de ordenar las letras de la palabra ANA? Total de letras: \\(n = 3\\) La letra A se repite dos veces: \\(n_1 = 2\\) La letra N aparece una vez: \\(n_2 = 1\\) \\[ P = \\frac{3!}{2! \\cdot 1!} = \\frac{6}{2} = 3 \\] Código en R COmo en el caso anterior, no existe una fórmula para las permutaciones. # Permutaciones con repetición: letras de &quot;ANA&quot; factorial(3) / (factorial(2) * factorial(1)) ## [1] 3 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
