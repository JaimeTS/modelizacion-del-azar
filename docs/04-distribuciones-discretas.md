# Modelos de probabilidad discretos {#discr}

Uno de los principales objetivos del C√°lculo de Probabilidades es construir modelos te√≥ricos que representen de forma adecuada el comportamiento de fen√≥menos aleatorios observables en la realidad. Estos modelos permiten al cient√≠fico simular situaciones, analizar comportamientos complejos y formular predicciones que, de otro modo, ser√≠an dif√≠ciles o imposibles de estudiar directamente. Modelizar lo observable no solo responde a una necesidad cient√≠fica b√°sica, sino que constituye una herramienta fundamental para entender y explicar el entorno que nos rodea, aunque ello implique necesariamente una simplificaci√≥n de la realidad.

En este tema nos centraremos en el estudio de los modelos de probabilidad discretos, es decir, aquellos que describen situaciones en las que las variables aleatorias solo pueden tomar un n√∫mero finito o numerable de valores posibles.

Comprender estos modelos es esencial porque proporcionan una base s√≥lida para analizar fen√≥menos tan variados como el n√∫mero de llamadas que recibe un centro de atenci√≥n, el resultado de experimentos repetidos o los patrones de demanda en un sistema econ√≥mico. Estudiar sus propiedades, estructura y aplicaciones nos permitir√° no solo interpretar correctamente situaciones aleatorias, sino tambi√©n tomar decisiones informadas en contextos reales basados en incertidumbre.

Este documento desarrolla los casos pr√°cticos de cada distribuci√≥n discreta, con teor√≠a, ejemplos aplicados al √°mbito econ√≥mico-empresarial y ejercicios resueltos, acompa√±ados de c√≥digo en R. El objetivo es proporcionar una herramienta completa que permita al alumno no solo comprender los fundamentos te√≥ricos de cada modelo, sino tambi√©n aplicarlos a problemas reales que surgen en el an√°lisis de datos econ√≥micos, financieros o empresariales.

## Distribuci√≥n uniforme discreta $(U_d)$

Una variable aleatoria discreta sigue una **distribuci√≥n uniforme discreta** cuando puede tomar un n√∫mero finito de valores distintos, todos ellos con **la misma probabilidad**. Es decir, cada uno de los posibles valores es **equiprobable**. Esta distribuci√≥n representa situaciones en las que no existe preferencia por ning√∫n resultado en particular, y todos los resultados posibles son igualmente probables.

**¬øPara qu√© sirve?** Se utiliza cuando todos los resultados son igualmente probables. En el contexto de empresa, sirve para modelar sorteos, selecci√≥n aleatoria de clientes, o cualquier proceso donde no hay sesgo hacia ning√∫n resultado.

**Teor√≠a:** Sea $X$ una variable aleatoria discreta que toma $n$ valores distintos: {$x_1, x_2, ..., x_n$} con igual probabilidad.

Se denota que la variable aleatoria $X$ sigue una distribuci√≥n uniforme de la siguiente manera 
$$
X \sim \mathcal{U}\_d(x_1, x_2, \dots, x_n)
$$
o bien, si los valores son enteros consecutivos entre dos extremos: 
$$
X \sim \mathcal{U}_d(a, b)
$$
donde $a$ y $b$ son enteros, y $X$ puede tomar cualquier valor en el conjunto $\{a, a+1, \dots, b\}$ con igual probabilidad.

En esta caso, su **funci√≥n de cuant√≠a** ser√°:
$$
P(X = x_i) = \frac{1}{n}, \\ i = 1, 2, ..., n.
$$

y su **funci√≥n de distribuci√≥n** (que representa la probabilidad acumulada) es:

$$
F(x_k) = P(X \leq x_k) = \sum_{i=1}^{k} \frac{1}{n} = \frac{k}{n}
$$

Esto da lugar a una funci√≥n escalonada, que puede expresarse como:

$$
F(x) =
\begin{cases}
0 & \text{si } x < x_1 \\\\
\frac{1}{n} & \text{si } x_1 \leq x < x_2 \\\\
\frac{2}{n} & \text{si } x_2 \leq x < x_3 \\\\
\vdots & \vdots \\\\
\frac{k}{n} & \text{si } x_k \leq x < x_{k+1} \\\\
\vdots & \vdots \\\\
1 & \text{si } x \geq x_n
\end{cases}
$$

**Par√°metros fundamentales:**

\- $n$: n√∫mero de posibles valores de la variable (positivo y entero).

**Momentos:**

\- Media: $E(X) = \frac{n + 1}{2}$

\- Varianza: $Var(X) = \frac{n^2 - 1}{12}$

**Ejercicio:** Se lanza un dado de 8 caras. ¬øCu√°l es la probabilidad de que salga un n√∫mero menor o igual a 3? Calcula su media y su varianza.

*Soluci√≥n:*

*- Probabilidad de obtener un n√∫mero menor o igual a 3*

Hay 3 resultados favorables: $\{1, 2, 3\}$, de un total de 8 posibles. Por tanto:

$$
P(X \leq 3) = \frac{3}{8} = 0{,}375
$$

*- Esperanza matem√°tica*

La esperanza de una distribuci√≥n uniforme discreta sobre $n$ valores es:

$$
E(X) = \frac{n + 1}{2} = \frac{8 + 1}{2} = \frac{9}{2} = 4{,}5
$$

*- Varianza*

La varianza se calcula con la f√≥rmula:

$$
V(X) = \frac{n^2 - 1}{12} = \frac{8^2 - 1}{12} = \frac{63}{12} = 5{,}25
$$

**Ejemplo en *R*:**

Consideremos un dado *perfecto* o *justo*, en el que la probabilidad de que salga cualquiera de sus caras es igual a $\frac{1}{6}$. Determ√≠nese la esperanza y la varianza de los posibles resultados que se pueden obtener.

La variable aleatoria $X$ puede tomar los valores $\{1, 2, 3, 4, 5, 6\}$, todos ellos con probabilidad $\frac{1}{6}$. Por tanto, puede modelizarse mediante una distribuci√≥n uniforme discreta.

Resoluci√≥n:


```r
# Probabilidad de obtener m√°s de 4
n_caras <- 6
prob_mayor_4 <- length(5:6) / n_caras
prob_mayor_4
```

```
## [1] 0.3333333
```

```r
# Media y varianza
media <- (n_caras + 1) / 2
varianza <- (n_caras^2 - 1) / 12
media
```

```
## [1] 3.5
```

```r
varianza
```

```
## [1] 2.916667
```

## Distribuci√≥n Bernoulli $(B(1, p))$

Una variable aleatoria discreta sigue una **distribuci√≥n de Bernoulli**, es decir, $\text{Binomial}(1, p)$, cuando modela una situaci√≥n en la que se realiza un √∫nico experimento aleatorio con dos posibles resultados complementarios, a los que com√∫nmente se denomina **√©xito** y **fracaso**,.

**¬øPara qu√© sirve?** Es muy habitual encontrar situaciones en las que se encuentre esta distribuci√≥n ya que modela experimentos de tipo √©xito/fracaso. Ejemplos t√≠picos en empresa ser√≠an si un cliente hace o no una compra, si una campa√±a publicitaria logra o no un clic, si una pieza sale defectuosa o no, etc.

**Teor√≠a:** Una variable aleatoria discreta $X$ sigue una **distribuci√≥n de Bernoulli** de par√°metro $p$, con $0 < p < 1$, si toma √∫nicamente los valores 0 y 1, que representan dos resultados complementarios de un √∫nico experimento aleatorio $$
X = \begin{cases} 1 & \text{con probabilidad } p \\ 0 & \text{con probabilidad } 1 - p \end{cases}
$$

Se denota como: $$
X \sim \text{Bernoulli}(p) \quad \text{o bien} \quad X \sim \text{Binomial}(1, p)
$$

Resumiendo la informaci√≥n

| Valor de $X$ | Probabilidad |
|--------------|--------------|
| $0$          | $1 - p$      |
| $1$          | $p$          |

De este modo, la **funci√≥n de cuant√≠a** o de probabilidad de $X$ es:

$$
f(x) = P(X = x) =
\begin{cases}
p & \text{si } x = 1 \\\\
1 - p & \text{si } x = 0 \\\\
0 & \text{en otro caso}
\end{cases}
$$

Tambi√©n puede escribirse de forma compacta como:

$$
f(x)=P(X = x) = p^x (1 - p)^{1 - x}, \quad \text{para } x \in \{0, 1\}
$$

Y la **Funci√≥n de Distribuci√≥n** o probabilidad acumulada $F(x) = P(X \leq x)$ es:

$$
F(x) =
\begin{cases}
0 & \text{si } x < 0 \\\\
1 - p & \text{si } 0 \leq x < 1 \\\\
1 & \text{si } x \geq 1
\end{cases}
$$

**Par√°metros fundamentales:**

-   $p \in [0,1]$: probabilidad de √©xito.

\- $q = 1 - p$: probabilidad de fracaso.

**Momentos:**

\- Media: $E(X) = p$

\- Varianza: $Var(X) = p(1 - p)$

**Ejercicio:** La probabilidad de que un cliente compre un producto tras recibir una oferta comercial es 0,3.

Determ√≠nese la media y la varianza de esta variable, y la probabilidad de que el cliente no realice la compra.

Dado que el fen√≥meno presenta dos posibles resultados ‚Äî**compra (C)** o **no compra (NC)**‚Äî, se puede modelizar mediante una distribuci√≥n binomial de un solo ensayo, es decir, $B(1; 0,3)$, donde:

$$
P(C) = P(X = 1) = p = 0{,}3 \\
P(NC) = P(X = 0) = q = 1 - p = 0{,}7
$$

La esperanza y la varianza de esta variable son:

$$
E(X) = p = 0{,}3 \\
V(X) = p \cdot q = 0{,}3 \cdot 0{,}7 = 0{,}21
$$

**Ejemplo en *R*: Simulaci√≥n**

Supongamos que la probabilidad de que un usuario haga clic en un anuncio es del 15‚ÄØ% (es decir, $p = 0{,}15$). Esta situaci√≥n se puede modelizar con una variable aleatoria de Bernoulli, donde el valor 1 (*√©xito*) representa que el usuario hace clic y el valor 0 (*fracaso*) que no lo hace. Vamos a simular el comportamiento de 10 individuos y posteriormente compararlo con el valor te√≥rico o esperado.

Para realizar esa simulaci√≥n del comportamiento de 10 usuarios utilizamos la funci√≥n `rbinom()` de R, y comparamos los resultados emp√≠ricos (media y varianza observadas) con los valores te√≥ricos esperados:


```r
set.seed(123)
resultados <- rbinom(10, size = 1, prob = 0.15)
resultados
```

```
##  [1] 0 0 0 1 1 0 0 1 0 0
```

```r
# Media y varianza emp√≠ricas
media_empirica <- mean(resultados); media_empirica
```

```
## [1] 0.3
```

```r
var_empirica <- var(resultados); var_empirica
```

```
## [1] 0.2333333
```

```r
# Valores te√≥ricos
p <- 0.15
media_teorica <- p
var_teorica <- p * (1 - p)
media_teorica
```

```
## [1] 0.15
```

```r
var_teorica
```

```
## [1] 0.1275
```

```r
dif_media <- media_empirica-media_teorica; dif_media
```

```
## [1] 0.15
```

```r
dif_var <- var_empirica-var_teorica; dif_var
```

```
## [1] 0.1058333
```

## Distribuci√≥n binomial $(B(n, p))$

Una variable aleatoria $X$ sigue una **distribuci√≥n binomial** con par√°metros $n$ y $p$, que se denota como $X \sim B(n; p)$, cuando puede expresarse como la suma de $n$ variables aleatorias independientes e id√©nticamente distribuidas seg√∫n una distribuci√≥n de Bernoulli $B(1, p)$, es decir:

$$
X = X_1 + X_2 + \cdots + X_n \quad \text{con} \quad X_i \overset{\text{iid}}{\sim} B(1; p)
$$

Desde un punto de vista conceptual, la distribuci√≥n $B(n; p)$ modeliza situaciones en las que un mismo experimento aleatorio dicot√≥mico (con dos posibles resultados: √©xito o fracaso) se repite $n$ veces bajo condiciones de independencia entre ensayos.

Dado que cada variable $X_i$ puede tomar solo los valores 0 o 1, la variable total $X$, que cuenta cu√°ntos √©xitos se han producido, puede tomar cualquier valor entero desde 0 (si todos los ensayos resultan en fracaso) hasta $n$ (si todos los ensayos resultan en √©xito). Por tanto, el rango de $X$ es:

$$
X \in \{0, 1, 2, \dots, n\}
$$

Al codificar el resultado "√©xito" como 1 y "fracaso" como 0, si la variable aleatoria $X$ toma el valor 3, significa que en las $n$ repeticiones se ha producido el suceso de inter√©s 3 veces y su complementario $n - 3$ veces. De este modo, $X$ funciona como una **variable contadora del n√∫mero de √©xitos** obtenidos en $n$ ensayos independientes, cada uno con probabilidad de √©xito $p$.

**¬øPara qu√© sirve?**

La distribuci√≥n binomial es una herramienta fundamental en la modelizaci√≥n de fen√≥menos aleatorios dicot√≥micos, es decir, aquellos en los que solo pueden ocurrir dos resultados mutuamente excluyentes (por ejemplo, √©xito o fracaso, compra o no compra, impago o pago, etc.).

Su principal uso es contar el n√∫mero de veces que ocurre un determinado resultado (denominado √©xito) en un n√∫mero fijo de repeticiones independientes de un mismo experimento aleatorio, cuando la probabilidad de √©xito se mantiene constante en cada ensayo.

En el √°mbito econ√≥mico y de la empresa, la distribuci√≥n binomial es especialmente √∫til para modelar situaciones como:

-   **An√°lisis de campa√±as de marketing**, donde se cuenta cu√°ntos clientes responden positivamente a una promoci√≥n, sabiendo que cada uno tiene una probabilidad conocida de hacerlo.
-   **Control de calidad**, para calcular la probabilidad de que un lote contenga un cierto n√∫mero de productos defectuosos.
-   **Estudios de mercado**, donde se estima el n√∫mero de consumidores que prefieren una marca entre varios entrevistados.
-   **Gesti√≥n del riesgo financiero**, al modelar el n√∫mero de impagos entre una cartera de cr√©ditos, cuando la probabilidad de impago es conocida y constante.
-   **Evaluaci√≥n del rendimiento de ventas**, como el n√∫mero de cierres exitosos entre un n√∫mero determinado de visitas comerciales.
-   **Toma de decisiones con incertidumbre**, al predecir resultados binarios en contextos repetidos como la aceptaci√≥n/rechazo de propuestas o la ocurrencia de eventos adversos.

Su simplicidad, interpretabilidad y aplicabilidad la convierten en una de las distribuciones m√°s utilizadas en an√°lisis de datos y en procesos de toma de decisiones bajo incertidumbre.

**Teor√≠a:**

Una variable aleatoria discreta $X$ se dice que sigue una **distribuci√≥n binomial** con par√°metros $n$ y $p$ si representa el n√∫mero de √©xitos obtenidos en $n$ ensayos independientes, cada uno con probabilidad de √©xito $p$. Se denota:

$$
X \sim B(n, p)
$$

donde:

-   $n \in \mathbb{N}$ es el n√∫mero de ensayos (experimentos independientes),
-   $p \in (0,1)$ es la probabilidad de √©xito en cada ensayo,
-   $X \in \{0, 1, 2, \dots, n\}$ representa el n√∫mero total de √©xitos.

La **funci√≥n de cuant√≠a** o funci√≥n de probabilidad de la distribuci√≥n binomial est√° dada por:

$$
P(X = x) = \binom{n}{x} p^x (1 - p)^{n - x}, \quad \text{para } x = 0, 1, 2, \dots, n
$$

donde:

-   $\binom{n}{x}$ es el coeficiente binomial, que cuenta el n√∫mero de formas de obtener $x$ √©xitos en $n$ ensayos,
-   $p^k$ representa la probabilidad de obtener $k$ √©xitos,
-   $(1 - p)^{n - x}$ representa la probabilidad de obtener $n - x$ fracasos.

Esta f√≥rmula permite calcular la probabilidad exacta de observar $x$ √©xitos en una secuencia de $n$ ensayos independientes con igual probabilidad de √©xito.

¬øC√≥mo se llega a esta funci√≥n de cuant√≠a?

La **funci√≥n de cuant√≠a** de una variable aleatoria binomial permite calcular la probabilidad de obtener exactamente $x$ √©xitos en $n$ ensayos independientes, cada uno con probabilidad de √©xito $p$ y de fracaso $q = 1 - p$.

Supongamos que realizamos $n$ repeticiones de un experimento aleatorio. Si en $x$ de esas repeticiones se obtiene el resultado deseado (√©xito), y en las $n - x$ restantes se obtiene el complementario (fracaso), la probabilidad de esa secuencia concreta es:

$$
\underbrace{p \cdot p \cdot \dots \cdot p}_{x \text{ veces}} \cdot \underbrace{q \cdot q \cdot \dots \cdot q}_{n - x \text{ veces}} = p^x q^{n - x}
$$

Pero esa es solo una de muchas formas posibles de obtener $x$ √©xitos y $n - x$ fracasos. Como el orden no importa, debemos contar cu√°ntas disposiciones distintas pueden formarse con $x$ √©xitos y $n - x$ fracasos. Esa cantidad viene dada por el **coeficiente binomial**:

$$
\binom{n}{x} = \frac{n!}{x!(n - x)!}
$$

Esto representa el n√∫mero de formas distintas en que podemos elegir $x$ posiciones para los √©xitos (valor 1) entre los $n$ ensayos.

De este modo, la probabilidad total de obtener exactamente $x$ √©xitos en $n$ ensayos es la funci√≥n de cuant√≠a previa.

**Par√°metros fundamentales:**

\- $n$: n√∫mero de ensayos.

\- $p$: probabilidad de √©xito.

**Momentos:**

\- Media: $E(X) = n p$

\- Varianza: $Var(X) = n p (1 - p)$

**Propiedad aditiva o reproductiva de la distribuci√≥n binomial**

La distribuci√≥n binomial es reproductiva respecto al par√°metro $n$. Esta propiedad establece que la suma de variables aleatorias independientes, cada una con distribuci√≥n binomial y el mismo par√°metro de √©xito $p$, tambi√©n sigue una distribuci√≥n binomial.

Formalmente, si:

$$
X \sim B(n_1, p), \quad Y \sim B(n_2, p), \quad \text{y} \quad X \perp Y
$$

entonces:

$$
X + Y \sim B(n_1 + n_2, p)
$$

Esta propiedad puede generalizarse a un n√∫mero finito de variables binomiales independientes que compartan el mismo par√°metro de probabilidad $p$.

La explicaci√≥n de esta propiedad se encuentra en la definici√≥n estructural de la binomial: una variable aleatoria $X \sim B(n, p)$ puede construirse como la suma de$n$ variables aleatorias independientes e id√©nticamente distribuidas seg√∫n una distribuci√≥n de Bernoulli$B(1, p)$:

$$
X = \sum_{i=1}^{n} \xi_i, \quad \text{con } \xi_i \overset{\text{iid}}{\sim} B(1, p)
$$

Dado que la suma de variables Bernoulli independientes con par√°metro $p$ genera una binomial, resulta inmediato que la suma de dos (o m√°s) variables binomiales independientes, cada una como suma de variables Bernoulli independientes, tambi√©n sigue una distribuci√≥n binomial, con n√∫mero total de ensayos igual a la suma de los ensayos de las variables originales.

**Ejercicios resueltos:**

**Ejercicio 1: Inspecci√≥n de productos**

Un inspector revisa 12 productos. La probabilidad de que un producto sea defectuoso es $p = 0.1$.

Se pide:

a\. Calcular la probabilidad de que exactamente 2 productos sean defectuosos.

b\. Determinar la media y la varianza del n√∫mero de productos defectuosos.

*Soluci√≥n:*

La variable aleatoria que cuenta el n√∫mero de defectuosos se modeliza como:

$$
X \sim B(12, 0.1)
$$

a)  Probabilidad de que$X = 2$:

$$
P(X = 2) = \binom{12}{2} \cdot 0.1^2 \cdot 0.9^{10} = 66 \cdot 0.01 \cdot 0.3487 \approx 0.2301
$$

b)  Media y varianza:

$$
\mathbb{E}(X) = 12 \cdot 0.1 = 1.2 \\
\operatorname{Var}(X) = 12 \cdot 0.1 \cdot 0.9 = 1.08
$$

**Ejercicio 2: Campa√±a de ventas en dos regiones**

Una empresa lanza una campa√±a de ventas en dos regiones. En la **Regi√≥n A**, se contacta a 10 clientes potenciales; en la **Regi√≥n B**, a 15 clientes. La probabilidad de que un cliente realice una compra tras el contacto es $p = 0.4$, y se asume que las decisiones de los clientes son independientes.

Se pide:

a\. Determinar la distribuci√≥n del n√∫mero total de compras realizadas en ambas regiones.

b\. Calcular la esperanza y la varianza del n√∫mero total de compras.

*Soluci√≥n:*

Sea:

$$
X \sim B(10, 0.4), \quad Y \sim B(15, 0.4)
$$

el n√∫mero de compras en cada regi√≥n. Como las variables son independientes y tienen el mismo valor de $p$, por la propiedad de **reproductividad** de la binomial, se tiene que:

$$
Z = X + Y \sim B(25, 0.4)
$$

a)  Esperanza:

$$
E(Z) = 25 \cdot 0.4 = 10
$$

b)  Varianza:

$$
\operatorname{Var}(Z) = 25 \cdot 0.4 \cdot 0.6 = 6
$$

Esto significa que, en promedio, se esperan 10 compras en total, con una variabilidad medida por la varianza de 6 (una variabilidad baja).

**Ejercicio en R: Inspecci√≥n de productos con defectos**

Se inspeccionan 15 productos y la probabilidad de que un producto sea defectuoso es del 5‚ÄØ% ($p = 0.05$).

Vamos a calcular:

-   La probabilidad de que haya exactamente 3 productos defectuosos.
-   La probabilidad de que haya **m√°s de 4** productos defectuosos.
-   La media y la varianza de la variable aleatoria $X \sim B(15, 0.05)$, que cuenta el n√∫mero de defectuosos.


```r
# Par√°metros
n <- 15
p <- 0.05

# a) Probabilidad de que haya exactamente 3 defectuosos
prob_3_defectuosos <- dbinom(3, size = n, prob = p)
prob_3_defectuosos
```

```
## [1] 0.03073298
```

```r
# b) Probabilidad de que haya m√°s de 4 defectuosos: P(X > 4) = 1 - P(X <= 4)
prob_mas_4_defectuosos <- 1 - pbinom(4, size = n, prob = p)
prob_mas_4_defectuosos
```

```
## [1] 0.0006146829
```

```r
# c) Media y varianza
media <- n * p
varianza <- n * p * (1 - p)
media
```

```
## [1] 0.75
```

```r
varianza
```

```
## [1] 0.7125
```



## Distribuci√≥n binomial negativa $(BN(r, p))$

Supongamos que realizamos un experimento de Bernoulli (con dos posibles resultados: √©xito o fracaso) de forma repetida hasta alcanzar un n√∫mero fijo de √©xitos, denotado por $r$.

Definimos la variable aleatoria $X$ como el **n√∫mero de fracasos que se producen antes de lograr el √©xito n√∫mero** $r$. En este contexto, decimos que:

$$
X \sim \operatorname{BN}(r, p)
$$

donde:

-   $r \in \mathbb{N}$ es el n√∫mero de √©xitos deseados,
-   $p \in (0,1)$ es la probabilidad de √©xito en cada ensayo,
-   $X \in \{0, 1, 2, \dots\}$ representa el n√∫mero de fracasos antes de lograr el $r$-√©simo √©xito.

Esta distribuci√≥n modeliza procesos donde el experimento se repite hasta alcanzar $r$ √©xitos, y nos interesa contar **cu√°ntos fracasos ocurren hasta ese momento (alcanzar el √©xito)**.

**¬øPara qu√© sirve?**

La distribuci√≥n binomial negativa se emplea cuando se desea modelar el n√∫mero de **fracasos necesarios para alcanzar un n√∫mero fijo de √©xitos** en experimentos de Bernoulli independientes, donde la probabilidad de √©xito se mantiene constante.

A diferencia de la distribuci√≥n binomial cl√°sica, donde se fija el n√∫mero de ensayos y se cuenta cu√°ntos √©xitos se obtienen, en la binomial negativa se fija el n√∫mero de √©xitos y se deja que el n√∫mero de ensayos (y por tanto de fracasos) var√≠e.

Esta distribuci√≥n resulta √∫til en distintos contextos, como por ejemplo:

-   **Marketing y ventas:** n√∫mero de contactos fallidos hasta lograr un n√∫mero determinado de ventas.
-   **Atenci√≥n al cliente:** n√∫mero de intentos fallidos antes de resolver satisfactoriamente $r$ casos.
-   **Recursos humanos:** n√∫mero de entrevistas no satisfactorias antes de contratar a $r$ personas adecuadas.
-   **Control de calidad:** n√∫mero de piezas defectuosas producidas antes de encontrar $r$ unidades sin defecto.
-   **Finanzas y riesgo:** n√∫mero de inversiones fallidas antes de lograr ùëü operaciones rentables.

En todos estos casos, el inter√©s est√° en **cu√°ntos intentos fallidos** ocurren antes de conseguir **un n√∫mero de √©xitos definidos**, lo cual es clave en an√°lisis de costos, eficiencia y planificaci√≥n.

**Teor√≠a:**

En este caso, la **funci√≥n de cuant√≠a** (funci√≥n de probabilidad) es:

$$
P(X = x) = \binom{x + r - 1}{x} \cdot p^r \cdot (1 - p)^x, \quad x = 0, 1, 2, \dots
$$

Esta f√≥rmula representa la probabilidad de que se observen exactamente $x$ fracasos antes de que ocurra el √©xito n√∫mero $r$, sin importar el orden en que ocurren los fracasos y los √©xitos intermedios.

**¬øC√≥mo se construye la funci√≥n de cuant√≠a?**

Suponiendo que denotamos a $A$ como √©xito y $A^*$ fracaso. Queremos calcular la probabilidad de que ocurran exactamente $x$ fracasos antes de que se produzca el √©xito n√∫mero $r$.

El suceso que describe la variable aleatoria $X$ se puede descomponer de la siguiente forma:

-   Se producen $r - 1$ √©xitos en alg√∫n orden, $A, A, \dots, A$
-   Seguidos de $x$ fracasos, $A^*, A^*, \dots, A^*$
-   Y finalmente el √©xito n√∫mero $r$, que cierra el experimento, $A$.

Este patr√≥n se representa como:

$$
\underbrace{A, A, \dots, A}_{r-1 \text{ veces}}, \quad \underbrace{A^*, A^*, \dots, A^*}_{x \text{ veces}}, \quad A
$$

La probabilidad de esta **secuencia concreta** (un √∫nico orden espec√≠fico) es:

$$
P(A, A, \dots, A, A^*, A^*, \dots, A^*, A) = p^{r-1} \cdot q^x \cdot p = p^r \cdot q^x
$$

Sin embargo, los $r - 1$ √©xitos y los $x$ fracasos pueden ocurrir **en cualquier orden** antes del √∫ltimo √©xito. Por tanto, necesitamos contar cu√°ntas disposiciones distintas de esos $r - 1$ √©xitos y $x$ fracasos existen en $x + r - 1$ posiciones.

Este n√∫mero de disposiciones se calcula mediante **permutaciones con repetici√≥n**, que dan lugar al **coeficiente binomial**:

$$
\binom{x + r - 1}{x} = \frac{(x + r - 1)!}{x! \cdot (r - 1)!}
$$

Finalmente, multiplicamos este n√∫mero de disposiciones por la probabilidad de cada una, obteniendo as√≠ la **funci√≥n de cuant√≠a** de la distribuci√≥n binomial negativa:

$$
P(X = x) = \binom{x + r - 1}{x} \cdot p^r \cdot q^x, \quad \text{para } x = 0, 1, 2, \dots
$$

**Par√°metros fundamentales:**

\- $r$: n√∫mero de √©xitos deseados.

\- $p$: probabilidad de √©xito.

**Momentos:**

\- Media: $E(X) = r\frac{q}{p}$

\- Varianza: $Var(X) = r\frac{(1 - p)}{p^2}$

**Ejercicio resuelto:**

Un operador comercial realiza llamadas a clientes. La probabilidad de que una llamada termine en una venta es $p = 0.2$. Se pide:

a\. Calcular la probabilidad de que el operador haga exactamente 4 llamadas sin √©xito antes de conseguir su tercera venta.

b\. Calcular la esperanza y la varianza del n√∫mero de fracasos antes de alcanzar 3 √©xitos.

*Soluci√≥n:*

Sea $X \sim \operatorname{BN}(r = 3, p = 0.2)$.\
a. Queremos calcular $P(X = 4)$.

La funci√≥n de cuant√≠a es:

$$
P(X = x) = \binom{x + r - 1}{x} \cdot p^r \cdot (1 - p)^x
$$

Sustituyendo los valores:

$$
P(X = 4) = \binom{6}{4} \cdot (0.2)^3 \cdot (0.8)^4 = 15 \cdot 0.008 \cdot 0.4096 \approx 0.0492
$$

La probabilidad de que el operador realice exactamente 4 llamadas fallidas antes de conseguir su tercera venta es aproximadamente 0,0492, es decir, un 4,92‚ÄØ%

b\. Esperanza y varianza:

Para una binomial negativa:

\- Esperanza:

$$
\mathbb{E}(X) = \frac{r(1 - p)}{p} = \frac{3 \cdot 0.8}{0.2} = 12
$$

Si este experimento se repitiera muchas veces (es decir, muchos operadores realizando llamadas en condiciones similares), el n√∫mero medio de llamadas sin √©xito antes de alcanzar la tercera venta tender√≠a a ser 12.

\- Varianza:

$$
\operatorname{Var}(X) = \frac{r(1 - p)}{p^2} = \frac{3 \cdot 0.8}{0.04} = 60
$$

**Ejercicio con R: Distribuci√≥n binomial negativa**

Una empresa realiza encuestas telef√≥nicas. La probabilidad de que una persona acepte responder la encuesta es $p = 0.25$.\

a.  ¬øCu√°l es la probabilidad de que se necesiten exactamente 5 rechazos antes de obtener la cuarta respuesta afirmativa?

b.  Calcula la esperanza y la varianza te√≥ricas.

c.  Simula 10,000 repeticiones del experimento y compara los resultados emp√≠ricos.

La variable aleatoria que cuenta el n√∫mero de rechazos (fracasos) antes de obtener 4 √©xitos se modeliza como:

$$
X \sim \operatorname{BN}(r = 4, p = 0.25)
$$


```r
# Par√°metros
r <- 4         # n√∫mero de √©xitos deseados
p <- 0.25      # probabilidad de √©xito
x <- 5         # n√∫mero de fracasos

#a. Probabilidad puntual
prob_5_fallos <- dnbinom(x, size = r, prob = p)
prob_5_fallos
```

```
## [1] 0.0519104
```

```r
#b. Esperanza y Varianza
esperanza <- r * (1 - p) / p
varianza <- r * (1 - p) / p^2
esperanza
```

```
## [1] 12
```

```r
varianza
```

```
## [1] 48
```

```r
#c. Simulaci√≥n de 10,000 repeticiones
set.seed(123)
simulaciones <- rnbinom(10000, size = r, prob = p)

# Media y varianza emp√≠ricas
mean(simulaciones)
```

```
## [1] 11.8066
```

```r
var(simulaciones)
```

```
## [1] 46.39244
```


## Distribuci√≥n Poisson $(\text{Poisson}(\lambda))$

Una variable aleatoria $X$ sigue una **distribuci√≥n de Poisson** con par√°metro $\lambda > 0$, que se denota como:

$$
X \sim \text{Poisson}(\lambda)
$$

cuando modeliza el n√∫mero de veces que ocurre un determinado suceso en un intervalo fijo de tiempo o espacio, siempre que:

-   Los sucesos ocurren de forma independiente,
-   La tasa media de ocurrencia $\lambda$ es constante en el tiempo o el espacio,
-   No hay ocurrencias simult√°neas (la probabilidad de m√°s de un suceso en un instante infinitesimal es despreciable).

Desde un punto de vista conceptual, la distribuci√≥n de Poisson **cuenta la cantidad de sucesos RAROS, discretos y aleatorios que ocurren en un intervalo determinado**.

Aunque, en teor√≠a, una variable aleatoria con distribuci√≥n de Poisson puede tomar cualquier valor natural, en la pr√°ctica las probabilidades de que adopte valores grandes disminuyen r√°pidamente a medida que estos crecen. Esto significa que, para un valor dado de $\lambda$, la mayor parte de la probabilidad se concentra en torno a unos pocos valores cercanos a la media, haciendo que tomar valores mucho mayores sea altamente improbable. Como consecuencia, la distribuci√≥n de Poisson resulta especialmente √∫til para describir sucesos que, aunque posibles, tienen una baja probabilidad de ocurrencia en gran n√∫mero dentro de un intervalo. Por este motivo, a la distribuci√≥n de Poisson tambi√©n se la conoce como la ***distribuci√≥n de los sucesos raros***.

El valor de $\lambda$ representa tanto la media como la varianza del n√∫mero de sucesos esperados en ese intervalo.

El soporte de la variable aleatoria es:

$$
X \in \{0, 1, 2, \dots\}
$$

**¬øPara qu√© sirve?**

La distribuci√≥n de Poisson es muy √∫til en la modelizaci√≥n de sucesos que:

-   Ocurren de forma aleatoria en el tiempo o en el espacio,
-   Son poco frecuentes o espor√°dicos, pero cuantificables.

Las aplicaciones m√°s habituales en econom√≠a y empresa

-   N√∫mero de llamadas recibidas en un centro de atenci√≥n por hora.
-   Llegada de clientes a una tienda o entidad financiera.
-   N√∫mero de errores de producci√≥n por lote o unidad de tiempo.
-   Incidencias log√≠sticas, como retrasos en entregas o fallos t√©cnicos por d√≠a.
-   Peticiones de pr√©stamos o reclamaciones recibidas en un periodo fijo.

Dado que solo requiere un par√°metro $\lambda$, es una distribuci√≥n muy vers√°til, especialmente √∫til cuando el n√∫mero de oportunidades de que ocurra el suceso no est√° bien definido, pero s√≠ su frecuencia media.

**Teor√≠a**

Una variable aleatoria discreta $X$ se dice que sigue una distribuci√≥n de Poisson con par√°metro $\lambda > 0$ si:

$$
X \sim \text{Poisson}(\lambda)
$$

donde:

-   $\lambda$: n√∫mero medio de sucesos que se espera que ocurran en un intervalo de tiempo (o espacio),
-   $X \in \{0, 1, 2, \dots\}$: n√∫mero de ocurrencias observadas.

**Funci√≥n de cuant√≠a**

La funci√≥n de cuant√≠a de la distribuci√≥n de Poisson es:

$$
P(X = x) = \frac{\lambda^x e^{-\lambda}}{x!}, \quad x = 0, 1, 2, \dots
$$

Esta f√≥rmula representa la probabilidad de que ocurran exactamente $x$ sucesos en un intervalo dado, cuando el n√∫mero medio de ocurrencias es $\lambda$.

**Par√°metros fundamentales**

\- $\lambda$: n√∫mero medio de ocurrencias del suceso en un intervalo (es la media y la varianza)

**Momentos**

-   Media: $E(X) = \lambda$

-   Varianza: $\operatorname{Var}(X) = \lambda$

**Propiedad aditiva o reproductiva de la *Poisson***

Sean las variables aleatorias independientes $( X_1, X_2, \dots, X_n )$, tales que cada $( X_j )$ sigue una distribuci√≥n de Poisson con par√°metro $( \lambda_j )$, es decir:

$$
X_j \sim \text{Poisson}(\lambda_j)
$$

Entonces, la **suma** de estas variables tambi√©n sigue una distribuci√≥n de Poisson.

Definimos:

$$
Y = X_1 + X_2 + \cdots + X_n
$$

Por tanto, la variable $Y$ tambi√©n sigue una distribuci√≥n de Poisson, con par√°metro:

$$
\lambda = \sum_{j=1}^{n} \lambda_j
$$

y se concluye que:

$$
Y \sim \text{Poisson} \left( \sum_{j=1}^{n} \lambda_j \right)
$$

Esta propiedad confirma que la **distribuci√≥n de Poisson es aditiva**: la suma de variables independientes que siguen Poisson tambi√©n es Poisson, siempre que los par√°metros se sumen.

*Ejemplo de la propiedad aditiva de la Poisson*

Supongamos que en una empresa:

-   El departamento A recibe, en promedio, 2 correos electr√≥nicos por hora.
-   El departamento B recibe, en promedio, 3 correos electr√≥nicos por hora.

Ambos procesos son independientes y se pueden modelar con variables aleatorias:

$$
X_1 \sim \text{Poisson}(2), \quad X_2 \sim \text{Poisson}(3)
$$

La variable $Y=X_1 + X_2$ representa el n√∫mero total de correos recibidos por ambos departamentos en una hora.

¬øCu√°l es la distribuci√≥n de $X_1 + X_2$? Por la propiedad aditiva de la distribuci√≥n de Poisson:

$$
Y=X_1 + X_2 \sim \text{Poisson}(2 + 3) = \text{Poisson}(5)
$$

**Ejercicios resueltos: Distribuci√≥n de Poisson**

**Ejercicio 1: N√∫mero de reclamaciones**

Una aseguradora recibe, en promedio, 3 reclamaciones por d√≠a. ¬øCu√°l es la probabilidad de que en un d√≠a se reciban exactamente 5 reclamaciones?

*Soluci√≥n:*

Modelamos la variable aleatoria con:

$$
X \sim \text{Poisson}(3)
$$

La probabilidad de que se reciban exactamente 5 reclamaciones, utulizando la funci√≥n de cuant√≠a de la Poisson es:

$$
P(X = 5) = \frac{3^5 \cdot e^{-3}}{5!} = \frac{243 \cdot e^{-3}}{120} \approx 0.1008
$$

En R:


```r
dpois(5, lambda = 3)
```

```
## [1] 0.1008188
```

**Ejercicio 2: Probabilidad de m√°s de 6 llamadas**

En una centralita telef√≥nica se reciben, de media, 4 llamadas por hora. ¬øCu√°l es la probabilidad de que en una hora se reciban m√°s de 6 llamadas?

*Soluci√≥n:*

La variable aleatoria que representa el n√∫mero de llamadas por hora se modeliza como:

$$
X \sim \text{Poisson}(4)
$$

Queremos calcular:

$$
P(X > 6) = 1 - P(X \leq 6)
$$

Primero calculamos $P(X \leq 6)$ sumando las probabilidades desde 0 hasta 6:

$$
P(X \leq 6) = \sum_{x = 0}^{6} \frac{4^x e^{-4}}{x!}
$$

Calculamos cada t√©rmino (aproximado):

$$
\begin{align*}
P(X = 0) &= \frac{4^0 e^{-4}}{0!} = e^{-4} \approx 0.0183 \\
P(X = 1) &= \frac{4^1 e^{-4}}{1!} = 4e^{-4} \approx 0.0733 \\
P(X = 2) &= \frac{4^2 e^{-4}}{2!} = 8e^{-4} \approx 0.1465 \\
P(X = 3) &= \frac{4^3 e^{-4}}{3!} = \frac{64e^{-4}}{6} \approx 0.1953 \\
P(X = 4) &= \frac{256e^{-4}}{24} \approx 0.1953 \\
P(X = 5) &= \frac{1024e^{-4}}{120} \approx 0.1563 \\
P(X = 6) &= \frac{4096e^{-4}}{720} \approx 0.1042 \\
\end{align*}
$$

Sumamos:

$$
P(X \leq 6) \approx 0.0183 + 0.0733 + 0.1465 + 0.1953 + 0.1953 + 0.1563 + 0.1042 = 0.8892
$$

Entonces:

$$
P(X > 6) = 1 - 0.8892 = 0.1108
$$

*Resoluci√≥n en R:*


```r
# Par√°metro de la Poisson

lambda <- 4

# Probabilidad de m√°s de 6 llamadas

prob_mas_6 <- ppois(6, lambda = lambda, lower.tail = FALSE); prob_mas_6
```

```
## [1] 0.110674
```

**Ejercicio con R: Gesti√≥n de solicitudes de pr√©stamos**

Un banco recibe, en promedio, 6 solicitudes de pr√©stamo por hora a trav√©s de su plataforma online. Este n√∫mero puede variar de hora en hora, pero se asume que las solicitudes llegan de forma independiente y a una tasa constante a lo largo del d√≠a. Por tanto, el n√∫mero de solicitudes por hora se modeliza mediante una distribuci√≥n de Poisson:

$$
X \sim \text{Poisson}(\lambda = 6)
$$

Se pide resolver con R las siguientes cuestiones:

1.  Calcular la probabilidad de que se reciban exactamente 8 solicitudes en una hora.\
2.  Calcular la probabilidad de que se reciban como m√°ximo 4 solicitudes en una hora.\
3.  Calcular la probabilidad de que se reciban m√°s de 10 solicitudes en una hora.\
4.  Simular lo que ocurrir√≠a durante una jornada laboral de 7 horas. ¬øCu√°ntas solicitudes se reciben en total?\
5.  Simular 10.000 jornadas laborales para estimar:
    -   La media y la varianza emp√≠rica del n√∫mero total de solicitudes por jornada,
    -   La proporci√≥n de jornadas en las que se reciben m√°s de 50 solicitudes, lo que se considera un nivel de saturaci√≥n operativa.


```r
# Par√°metro por hora
lambda <- 6

#1. probabilidad de que se reciban exactamente 8 solicitudes en una hora
prob_8 <- dpois(8, lambda = lambda)
prob_8
```

```
## [1] 0.1032577
```

```r
#2. probabilidad de que se reciban como m√°ximo 4 solicitudes en una hora.  

prob_4_o_menos <- ppois(4, lambda = lambda)
prob_4_o_menos
```

```
## [1] 0.2850565
```

```r
#3. probabilidad de que se reciban m√°s de 10 solicitudes en una hora.  

prob_mas_10 <- 1 - ppois(10, lambda = lambda)
prob_mas_10
```

```
## [1] 0.04262092
```

```r
#4.  Simular lo que ocurrir√≠a durante una jornada laboral de 7 horas. ¬øCu√°ntas solicitudes se reciben en total?  
set.seed(123)  # para reproducibilidad
jornada <- sum(rpois(7, lambda = lambda))
jornada
```

```
## [1] 45
```

```r
#5  Simular 10.000 jornadas laborales para estimar:

sim_jornadas <- replicate(10000, sum(rpois(7, lambda = lambda)))

#5.1 La media y la varianza emp√≠rica del n√∫mero total de solicitudes por jornada,
   
# Media emp√≠rica
mean(sim_jornadas)
```

```
## [1] 41.8975
```

```r
# Varianza emp√≠rica
var(sim_jornadas)
```

```
## [1] 41.86078
```

```r
#5.2 La proporci√≥n de jornadas en las que se reciben m√°s de 50 solicitudes, lo que se considera un nivel de saturaci√≥n operativa.

mean(sim_jornadas > 50)
```

```
## [1] 0.0946
```

## Distribuci√≥n geom√©trica $(G(p))$

Una variable aleatoria $X$ sigue una distribuci√≥n geom√©trica con par√°metro $p$, que se denota como:

$$
X \sim G(p)
$$

cuando representa el n√∫mero de **ensayos necesarios hasta la obtenci√≥n del primer √©xito** en una secuencia de ensayos de Bernoulli independientes, cada uno con probabilidad de √©xito $p$ y fracaso $q = 1 - p$.

Desde un punto de vista conceptual, la distribuci√≥n geom√©trica modeliza situaciones en las que se repite un mismo experimento aleatorio dicot√≥mico hasta que se produce el primer √©xito.

En este contexto, $X$ toma valores en el conjunto:

$$
X \in \{1, 2, 3, \dots\}
$$

Su interpretaci√≥n es clara: si $X = 4$, significa que han sido necesarios cuatro intentos para obtener el primer √©xito, y que los tres primeros intentos fueron fracasos.

**¬øPara qu√© sirve?**

La distribuci√≥n geom√©trica es √∫til en contextos donde se desea modelar el n√∫mero de intentos necesarios hasta obtener un resultado positivo. Algunas aplicaciones en econom√≠a y empresa son:

-   **Tiempos de espera en servicios**: n√∫mero de clientes atendidos hasta lograr una venta.
-   **Modelos de respuesta de clientes**: n√∫mero de llamadas o correos hasta obtener respuesta.
-   **Gesti√≥n de inventarios**: n√∫mero de periodos sin ventas hasta que se produce una compra.
-   **Riesgo crediticio**: n√∫mero de cr√©ditos concedidos hasta observar el primer impago.

Su interpretaci√≥n intuitiva y su estructura simple la hacen muy √∫til en modelos probabil√≠sticos b√°sicos y en simulaciones.

**Teor√≠a**

Una variable aleatoria discreta $X$ se distribuye geom√©tricamente con par√°metro $p \in (0,1)$ si modeliza el n√∫mero de intentos necesarios hasta obtener el primer √©xito. Se denota:

$$
X \sim G(p)
$$

donde:

-   $p$ es la probabilidad de √©xito en cada ensayo.
-   $q = 1 - p$ es la probabilidad de fracaso.
-   $X \in \{1, 2, 3, \dots\}$

**Funci√≥n de cuant√≠a**

La funci√≥n de cuant√≠a (funci√≥n de probabilidad) de la distribuci√≥n geom√©trica es:

$$
P(X = x) = q^{x - 1} \cdot p, \quad \text{para } x = 1, 2, 3, \dots
$$

Esta f√≥rmula representa la probabilidad de que se produzcan $x - 1$ fracasos seguidos antes de obtener el primer √©xito.

**Par√°metros fundamentales**

-   Media: $E(X) = \frac{1}{p}$

-   Varianza:$\text{Var}(X) = \frac{1 - p}{p^2}$

**Ejercicios resueltos: Distribuci√≥n Geom√±etrica**

**Ejercicio 1**

Un agente comercial realiza llamadas a posibles clientes. La probabilidad de que una llamada termine en una venta es $p = 0.2$ ¬øCu√°l es la probabilidad de que la primera venta se consiga en el cuarto intento?

Soluci√≥n:

Sea $X \sim G(0.2)$, donde $X$ representa el n√∫mero de llamadas hasta la primera venta.

Queremos calcular:

$$
P(X = 4) = q^{4 - 1} \cdot p = (1 - 0.2)^3 \cdot 0.2 = 0.8^3 \cdot 0.2 = 0.512 \cdot 0.2 = 0.1024
$$

Interpretaci√≥n: Existe aproximadamente un 10,24% de probabilidad de que el agente realice tres llamadas fallidas y consiga una venta en el cuarto intento.

**Ejercicio 2: Resoluci√≥n con R**

Sup√≥n ahora que la probabilidad de √©xito es $p = 0.1$. Se pide:

1.  Calcular la probabilidad de que la primera venta se logre en el sexto intento.\
2.  Calcular la probabilidad de que se logre en 6 o menos intentos.\
3.  Calcular la media y la varianza de la distribuci√≥n.\
4.  Simular 10.000 experimentos y estimar la media emp√≠rica.



```r
# Par√°metro
p <- 0.1

# 1. P(X = 6)
prob_6 <- dgeom(6 - 1, prob = p)  # en R, la geom√©trica cuenta fracasos antes del primer √©xito
prob_6
```

```
## [1] 0.059049
```

```r
# 2. P(X <= 6)
prob_acum_6 <- pgeom(6 - 1, prob = p)
prob_acum_6
```

```
## [1] 0.468559
```

```r
# 3. Media y varianza te√≥ricas
media <- 1 / p
varianza <- (1 - p) / (p^2)
media
```

```
## [1] 10
```

```r
varianza
```

```
## [1] 90
```

```r
# 4. Simulaci√≥n de 10.000 experimentos
set.seed(123)
sim <- rgeom(10000, prob = p) + 1  # sumamos 1 porque R devuelve fracasos
mean(sim)
```

```
## [1] 9.8619
```

## Distribuci√≥n hipergeom√©trica $(H(N, K, n))$

Una variable aleatoria discreta $X$ sigue una **distribuci√≥n hipergeom√©trica** cuando representa el n√∫mero de √©xitos obtenidos al seleccionar una muestra aleatoria **sin reemplazo** de una poblaci√≥n finita que contiene un n√∫mero determinado de elementos exitosos y no exitosos.

Se denota como:

$$
X \sim H(N, K, n)
$$

donde:

-   $N$: tama√±o total de la poblaci√≥n,
-   $K$: n√∫mero total de elementos exitosos en la poblaci√≥n,
-   $n$: tama√±o de la muestra extra√≠da sin reemplazo,
-   $X$: n√∫mero de √©xitos en la muestra.

El soporte de $X$ est√° dado por los valores enteros que cumplen:

$$
\max(0, n - (N - K)) \leq X \leq \min(n, K)
$$

**¬øPara qu√© sirve?**

La distribuci√≥n hipergeom√©trica es √∫til para modelar situaciones de muestreo **sin reemplazo**, muy habituales en contextos reales donde la proporci√≥n de √©xitos cambia con cada extracci√≥n.

**Aplicaciones en econom√≠a y empresa:**

-   **Control de calidad**: extraer productos sin reemplazo para evaluar cu√°ntos son defectuosos.
-   **Auditor√≠as**: verificar cu√°ntas facturas con errores hay en una muestra de registros contables.
-   **Inspecci√≥n de lotes en inventarios**.
-   **Selecci√≥n de consumidores** para una campa√±a donde algunos ya han sido contactados.
-   **Evaluaci√≥n del fraude** o incumplimiento en una muestra finita de expedientes.

**Teor√≠a**

Sea una poblaci√≥n de tama√±o $N$, con $K$ elementos exitosos y $N - K$ fracasos. Si se extrae una muestra aleatoria de tama√±o $n$, sin reemplazo, la variable aleatoria $X$ que representa el n√∫mero de √©xitos en la muestra se distribuye como:

$$
X \sim H(N, K, n)
$$

**Funci√≥n de cuant√≠a**

La funci√≥n de cuant√≠a (funci√≥n de probabilidad) de la distribuci√≥n hipergeom√©trica es:

$$
P(X = x) = \frac{\binom{K}{x} \binom{N - K}{n - x}}{\binom{N}{n}}, \quad \text{para } x = \max(0, n - (N - K)), \dots, \min(n, K)
$$

Esta f√≥rmula expresa el cociente entre:

-   El n√∫mero de formas de seleccionar $x$ √©xitos entre los $K$ existentes y $n - x$ fracasos entre los $N - K$ restantes,
-   y el n√∫mero total de formas de seleccionar $n$ elementos de entre los $N$.

**Par√°metros fundamentales**

-   $N$: tama√±o de la poblaci√≥n,
-   $K$: n√∫mero total de √©xitos en la poblaci√≥n,
-   $n$: tama√±o de la muestra.

**Momentos**

**- Media:** $E(X) = n \cdot \frac{K}{N}$

**- Varianza**:

$\text{Var}(X) = n \cdot \frac{K}{N} \cdot \frac{N - K}{N} \cdot \frac{N - n}{N - 1}$

**Ejercicios**

**Ejercicio 1:** Un lote contiene 20 productos, de los cuales 6 son defectuosos. Se selecciona una muestra aleatoria de 5 productos sin reemplazo. ¬øCu√°l es la probabilidad de que haya exactamente 2 productos defectuosos en la muestra?

*Soluci√≥n:*

Sea $X \sim H(20, 6, 5)$, y queremos calcular $P(X = 2)$. Aplicamos la funci√≥n de cuant√≠a:

$$
P(X = 2) = \frac{\binom{6}{2} \binom{14}{3}}{\binom{20}{5}} = \frac{15 \cdot 364}{15504} \approx 0.352
$$

*Interpretaci√≥n:*\
Existe un 35.2‚ÄØ% de probabilidad de que exactamente 2 de los 5 productos seleccionados sean defectuosos.

**Ejercicio 2 en R.**

Supongamos que se tiene una poblaci√≥n de 50 elementos, de los cuales 12 son considerados "√©xito". Se toma una muestra de tama√±o 10 sin reemplazo. Se pide:

1.  Calcular la probabilidad de obtener exactamente 3 √©xitos.
2.  Calcular la probabilidad de obtener 3 o menos √©xitos.
3.  Calcular la esperanza y la varianza te√≥ricas.


```r
# Par√°metros
N <- 50   # poblaci√≥n total
K <- 12   # n√∫mero de √©xitos en la poblaci√≥n
n <- 10   # tama√±o de la muestra
x <- 3

# 1. Probabilidad de obtener exactamente 3 √©xitos
dhyper(x, m = K, n = N - K, k = n)
```

```
## [1] 0.2702863
```

```r
# 2. Probabilidad de obtener 3 o menos √©xitos
phyper(x, m = K, n = N - K, k = n)
```

```
## [1] 0.8209435
```

```r
# 3. Media y varianza te√≥ricas
media <- n * K / N
varianza <- n * (K / N) * ((N - K) / N) * ((N - n) / (N - 1))
media
```

```
## [1] 2.4
```

```r
varianza
```

```
## [1] 1.48898
```

## Distribuci√≥n multinomial $(M(n; {p_i}))$

La distribuci√≥n multinomial generaliza la distribuci√≥n binomial a m√°s de dos categor√≠as posibles. Una variable aleatoria $\boldsymbol{X} = (X_1, X_2, \dots, X_k)$ sigue una **distribuci√≥n multinomial** cuando describe el n√∫mero de veces que ocurren $k$ posibles resultados mutuamente excluyentes tras realizar $n$ ensayos independientes, cada uno con las mismas probabilidades.

Se denota como:

$$
(X_1, X_2, \dots, X_k) \sim \text{Multinomial}(n; p_1, p_2, \dots, p_k)
$$

o

$$
(X_1, X_2, \dots, X_k) \sim M(n; p_1, p_2, \dots, p_k)
$$

donde $n$ es el n√∫mero total de repeticiones y $p_j$ son las probabilidades de que ocurra cada una de las posibles categor√≠as o clases.

**¬øPara qu√© sirve?**

La distribuci√≥n multinomial resulta especialmente √∫til cuando se desea modelar situaciones en las que un experimento puede arrojar **m√°s de dos posibles resultados excluyentes** y se repite un n√∫mero fijo de veces bajo condiciones de independencia. A diferencia de la binomial, que solo contempla dos categor√≠as (√©xito/fracaso), la multinomial permite trabajar con **tres o m√°s categor√≠as**

En el √°mbito econ√≥mico y empresarial, su utilidad es muy amplia. Por ejemplo:

-   **Investigaci√≥n de mercados**: permite modelar c√≥mo se distribuyen las preferencias de los consumidores entre distintas marcas, productos o servicios. Si una empresa desea saber c√≥mo se reparte el mercado entre varias marcas competidoras, la distribuci√≥n multinomial ofrece una base probabil√≠stica s√≥lida para estimarlo.

-   **An√°lisis de comportamiento del consumidor**: se puede emplear para estudiar la distribuci√≥n de respuestas en una encuesta donde los participantes deben elegir una sola opci√≥n entre varias (por ejemplo, tipo de transporte utilizado, canal de compra preferido, nivel de satisfacci√≥n, etc.).

-   **Gesti√≥n de inventarios o log√≠stica**: es √∫til para modelar c√≥mo se reparte la demanda entre distintas categor√≠as de productos en una tienda o c√≥mo se distribuyen los env√≠os entre diferentes zonas geogr√°ficas.

-   **Evaluaci√≥n de riesgos financieros o crediticios**: permite estudiar c√≥mo se clasifican los clientes en diferentes niveles de solvencia o riesgo.

**Teor√≠a**

Supongamos un experimento con $k$ resultados posibles mutuamente excluyentes: $X_1, X_2, \dots, X_k$, con:

$$
P(X_j) = p_j \quad \text{para todo } j = 1, \dots, k, \quad \text{y } \sum_{j=1}^{k} p_j = 1
$$

Si se repite el experimento $n$ veces de forma independiente, y se observa $x_j$ veces el resultado $X_j$, el vector aleatorio:

$$
(x_1, x_2, \dots, x_k)
$$

sigue una distribuci√≥n multinomial.

**Funci√≥n de cuant√≠a**

La funci√≥n de probabilidad conjunta es:

$$
P(X_1 = x_1, X_2 = x_2, \dots, X_k = x_k) = \frac{n!}{x_1! \cdot x_2! \cdots x_k!} \cdot p_1^{x_1} \cdot p_2^{x_2} \cdots p_k^{x_k}
$$

para todo $x_1 + x_2 + \cdots + x_k = n$.

donde:

-   $n$: n√∫mero total de repeticiones del experimento aleatorio,
-   $p_j \in (0,1)$: probabilidad de que ocurra el resultado $X_j$,
-   $\sum_{j=1}^{k} p_j = 1$,
-   $x_j$: n√∫mero de veces que se observa el resultado $X_j$,
-   $\sum_{j=1}^{k} x_j = n$.

**Par√°metros fundamentales**

-   $n \in \mathbb{N}$: n√∫mero de ensayos.
-   $p_1, p_2, \dots, p_k \in (0, 1)$ tales que $\sum_{j=1}^k p_j = 1$

**Momentos**

-   $E(X_j) = n \cdot p_j$
-   $\text{Var}(X_j) = n \cdot p_j (1 - p_j)$
-   $\text{Cov}(X_i, X_j) = -n \cdot p_i \cdot p_j \quad \text{para } i \neq j$

**Ejercicio**

En una encuesta, se pregunta a 10 personas por su preferencia entre tres marcas de tel√©fono: A, B y C. Las probabilidades de preferencia son $p_A = 0.5$, $p_B = 0.3$ y $p_C = 0.2$ ¬øCu√°l es la probabilidad de que exactamente 4 personas prefieran A, 3 prefieran B y 3 prefieran C?

*Soluci√≥n:*

Aplicamos la funci√≥n de cuant√≠a:

$$
P(X_A = 4, X_B = 3, X_C = 3) = \frac{10!}{4! \cdot 3! \cdot 3!} \cdot 0.5^4 \cdot 0.3^3 \cdot 0.2^3
$$

Calculamos paso a paso:

$$
\frac{10!}{4!3!3!} = \frac{3628800}{24 \cdot 6 \cdot 6} = \frac{3628800}{864} = 4200
$$

$$
P = 4200 \cdot 0.0625 \cdot 0.027 \cdot 0.008 = 4200 \cdot 0.0000135 \approx 0.0567
$$

**Ejemplo en R**

Vamos a calcular la probabilidad anterior usando R.


```r
# Carga de paquete necesario
#install.packages("gtools") # si no lo tienes instalado
library(gtools)
```

```
## Warning: package 'gtools' was built under R version 4.3.3
```

```r
# Par√°metros
n <- 10
x <- c(4, 3, 3)
p <- c(0.5, 0.3, 0.2)

# Probabilidad multinomial
dmultinom(x, prob = p)
```

```
## [1] 0.0567
```

## Resumen de las distribuciones discretas

| Distribuci√≥n | Variable | Par√°metros | Espacio muestral | Funci√≥n de cuant√≠a | Media | Varianza |
|-----------|-----------|-----------|-----------|-----------|-----------|-----------|
| **Binomial** | N√∫mero de √©xitos en $n$ ensayos | $n \in \mathbb{N},\ p \in (0,1)$ | $\{0, 1, \dots, n\}$ | $P(X = x) = \binom{n}{x} p^x (1 - p)^{n - x}$ | $np$ | $np(1 - p)$ |
| **Bernoulli** | √âxito o fracaso (1 experimento) | $p \in (0,1)$ | $\{0, 1\}$ | $P(X = x) = p^x (1 - p)^{1 - x}$ | $p$ | $p(1 - p)$ |
| **Poisson** | N√∫mero de eventos en un intervalo | $\lambda > 0$ | $\mathbb{N}_0$ | $P(X = x) = \dfrac{\lambda^x e^{-\lambda}}{x!}$ | $\lambda$ | $\lambda$ |
| **Geom√©trica** | Ensayos hasta el 1er √©xito | $p \in (0,1)$ | $\{1, 2, 3, \dots\}$ | $P(X = x) = (1 - p)^{x - 1} p$ | $\frac{1}{p}$ | $\frac{1 - p}{p^2}$ |
| **Binomial negativa** | Ensayos hasta $r$-√©simo √©xito | $r \in \mathbb{N},\ p \in (0,1)$ | $\{r, r + 1, \dots\}$ | $P(X = x) = \binom{x - 1}{r - 1} p^r (1 - p)^{x - r}$ | $\frac{r}{p}$ | $\frac{r(1 - p)}{p^2}$ |
| **Hipergeom√©trica** | √âxitos en muestra sin reemplazo | $N,\ K,\ n \in \mathbb{N}$ | $\max(0, n - (N - K)) \leq x \leq \min(n, K)$ | $P(X = x) = \dfrac{\binom{K}{x} \binom{N - K}{n - x}}{\binom{N}{n}}$ | $n \cdot \frac{K}{N}$ | $n \cdot \frac{K}{N} \cdot \frac{N - K}{N} \cdot \frac{N - n}{N - 1}$ |
| **Multinomial** | Recuento de $k$ categor√≠as en $n$ ensayos | $n \in \mathbb{N},\ p_1 + \cdots + p_k = 1$ | $\{(x_1, \dots, x_k): \sum x_j = n\}$ | $P(X_1 = x_1, \dots, X_k = x_k) = \dfrac{n!}{x_1!\cdots x_k!} p_1^{x_1} \cdots p_k^{x_k}$ | $\mathbb{E}(X_j) = np_j$ | $\text{Var}(X_j) = np_j(1 - p_j)$, $\text{Cov}(X_i, X_j) = -np_i p_j$ |

Notas:

-   $\mathbb{N}$ denota los enteros positivos, y $\mathbb{N}_0 = \{0, 1, 2, \dots\}$.
-   La **multinomial** se interpreta como la generalizaci√≥n de la binomial para m√°s de dos categor√≠as.

## Funciones disponibles en R para funciones discretas

Para cada distribuci√≥n discreta se tienen 4 funciones, a continuaci√≥n el listado de funciones y su utilidad.


```r
dxxx(x, ...)  # Funci√≥n de masa de probabilidad, f(x)
pxxx(q, ...)  # Funci√≥n de distribuci√≥n acumulada hasta q, F(x)
qxxx(p, ...)  # Cuantil para el cual P(X <= q) = p
rxxx(n, ...)  # Generador de n√∫meros aleatorios.
```

En el lugar de las letras `xxx` se de debe colocar el nombre de la distribuci√≥n en R, a continuaci√≥n el listado de nombres disponibles para las 6 distribuciones discretas b√°sicas.


```r
binom     # Binomial
geo       # Geom√©trica
nbinom    # Binomial negativa
hyper     # Hipergeom√©trica
pois      # Poisson
multinom  # Multinomial
```

Combinando las funciones y los nombres se tiene un total de 24 funciones, por ejemplo, para obtener la funci√≥n de masa de probabilidad $f(x)$ de una binomial se usa la funci√≥n `dbinom( )` y para obtener la funci√≥n acumulada $F(x)$ de una Poisson se usa la funci√≥n `ppois( )`.
